{
    "journal": {
        "id": "eLife", 
        "title": "eLife", 
        "issn": "2050-084X"
    }, 
    "snippet": {
        "status": "vor", 
        "id": "14521", 
        "version": 2, 
        "type": "research-article", 
        "doi": "10.7554/eLife.14521", 
        "authorLine": "Hyojin Park et al", 
        "title": "Lip movements entrain the observers\u2019 low-frequency brain oscillations to facilitate speech intelligibility", 
        "published": "2016-05-05T00:00:00Z", 
        "volume": 5, 
        "elocationId": "e14521", 
        "pdf": "https://publishing-cdn.elifesciences.org/14521/elife-14521-v2.pdf", 
        "subjects": [
            {
                "id": "neuroscience", 
                "name": "Neuroscience"
            }
        ], 
        "researchOrganisms": [
            "Human"
        ], 
        "abstract": {
            "doi": "10.7554/eLife.14521.001", 
            "content": [
                {
                    "type": "paragraph", 
                    "text": "During continuous speech, lip movements provide visual temporal signals that facilitate speech processing. Here, using MEG we directly investigated how these visual signals interact with rhythmic brain activity in participants listening to and seeing the speaker. First, we investigated coherence between oscillatory brain activity and speaker\u2019s lip movements and demonstrated significant entrainment in visual cortex. We then used partial coherence to remove contributions of the coherent auditory speech signal from the lip-brain coherence. Comparing this synchronization between different attention conditions revealed that attending visual speech enhances the coherence between activity in visual cortex and the speaker\u2019s lips. Further, we identified a significant partial coherence between left motor cortex and lip movements and this partial coherence directly predicted comprehension accuracy. Our results emphasize the importance of visually entrained and attention-modulated rhythmic brain activity for the enhancement of audiovisual speech processing."
                }
            ]
        }, 
        "copyright": {
            "license": "CC-BY-4.0", 
            "holder": "Park et al", 
            "statement": "This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited."
        }, 
        "authors": [
            {
                "type": "person", 
                "name": {
                    "preferred": "Hyojin Park", 
                    "index": "Park, Hyojin"
                }, 
                "orcid": "0000-0002-7527-8280", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "Hyojin.Park@glasgow.ac.uk"
                ], 
                "contribution": "HP, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Christoph Kayser", 
                    "index": "Kayser, Christoph"
                }, 
                "orcid": "0000-0001-7362-5704", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "contribution": "CK, Conception and design, Analysis and interpretation of data, Drafting or revising the article", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Gregor Thut", 
                    "index": "Thut, Gregor"
                }, 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "contribution": "GT, Conception and design, Analysis and interpretation of data, Drafting or revising the article", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Joachim Gross", 
                    "index": "Gross, Joachim"
                }, 
                "orcid": "0000-0002-3994-1006", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "Joachim.Gross@glasgow.ac.uk"
                ], 
                "contribution": "JG, Conception and design, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents", 
                "competingInterests": "The authors declare that no competing interests exist."
            }
        ], 
        "ethics": [
            {
                "type": "paragraph", 
                "text": "Human subjects: This study was approved by the local ethics committee (CSE01321; University of Glasgow, College of Science and Engineering) and conducted in conformity with the Declaration of Helsinki. All participants provided informed written consent before participating in the experiment and received monetary compensation for their participation."
            }
        ], 
        "funding": {
            "awards": [
                {
                    "id": "par-1", 
                    "source": {
                        "funderId": "10.13039/501100000268", 
                        "name": [
                            "Biotechnology and Biological Sciences Research Council"
                        ]
                    }, 
                    "awardId": "BB/L027534/1", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Christoph Kayser", 
                                "index": "Kayser, Christoph"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-2", 
                    "source": {
                        "funderId": "10.13039/501100000781", 
                        "name": [
                            "European Research Council"
                        ]
                    }, 
                    "awardId": "646657", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Christoph Kayser", 
                                "index": "Kayser, Christoph"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-3", 
                    "source": {
                        "funderId": "10.13039/100004440", 
                        "name": [
                            "Wellcome Trust"
                        ]
                    }, 
                    "awardId": "098433", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Joachim Gross", 
                                "index": "Gross, Joachim"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-4", 
                    "source": {
                        "funderId": "10.13039/100004440", 
                        "name": [
                            "Wellcome Trust"
                        ]
                    }, 
                    "awardId": "098434", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Joachim Gross", 
                                "index": "Gross, Joachim"
                            }
                        }
                    ]
                }
            ], 
            "statement": "The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication."
        }, 
        "impactStatement": "Rhythms of lip movements entrain low-frequency brain oscillations and facilitate audiovisual speech processing."
    }, 
    "article": {
        "status": "vor", 
        "id": "14521", 
        "version": 2, 
        "type": "research-article", 
        "doi": "10.7554/eLife.14521", 
        "authorLine": "Hyojin Park et al", 
        "title": "Lip movements entrain the observers\u2019 low-frequency brain oscillations to facilitate speech intelligibility", 
        "published": "2016-05-05T00:00:00Z", 
        "volume": 5, 
        "elocationId": "e14521", 
        "pdf": "https://publishing-cdn.elifesciences.org/14521/elife-14521-v2.pdf", 
        "subjects": [
            {
                "id": "neuroscience", 
                "name": "Neuroscience"
            }
        ], 
        "researchOrganisms": [
            "Human"
        ], 
        "abstract": {
            "doi": "10.7554/eLife.14521.001", 
            "content": [
                {
                    "type": "paragraph", 
                    "text": "During continuous speech, lip movements provide visual temporal signals that facilitate speech processing. Here, using MEG we directly investigated how these visual signals interact with rhythmic brain activity in participants listening to and seeing the speaker. First, we investigated coherence between oscillatory brain activity and speaker\u2019s lip movements and demonstrated significant entrainment in visual cortex. We then used partial coherence to remove contributions of the coherent auditory speech signal from the lip-brain coherence. Comparing this synchronization between different attention conditions revealed that attending visual speech enhances the coherence between activity in visual cortex and the speaker\u2019s lips. Further, we identified a significant partial coherence between left motor cortex and lip movements and this partial coherence directly predicted comprehension accuracy. Our results emphasize the importance of visually entrained and attention-modulated rhythmic brain activity for the enhancement of audiovisual speech processing."
                }
            ]
        }, 
        "copyright": {
            "license": "CC-BY-4.0", 
            "holder": "Park et al", 
            "statement": "This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited."
        }, 
        "authors": [
            {
                "type": "person", 
                "name": {
                    "preferred": "Hyojin Park", 
                    "index": "Park, Hyojin"
                }, 
                "orcid": "0000-0002-7527-8280", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "Hyojin.Park@glasgow.ac.uk"
                ], 
                "contribution": "HP, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Christoph Kayser", 
                    "index": "Kayser, Christoph"
                }, 
                "orcid": "0000-0001-7362-5704", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "contribution": "CK, Conception and design, Analysis and interpretation of data, Drafting or revising the article", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Gregor Thut", 
                    "index": "Thut, Gregor"
                }, 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "contribution": "GT, Conception and design, Analysis and interpretation of data, Drafting or revising the article", 
                "competingInterests": "The authors declare that no competing interests exist."
            }, 
            {
                "type": "person", 
                "name": {
                    "preferred": "Joachim Gross", 
                    "index": "Gross, Joachim"
                }, 
                "orcid": "0000-0002-3994-1006", 
                "affiliations": [
                    {
                        "name": [
                            "Institute of Neuroscience and Psychology", 
                            "University of Glasgow"
                        ], 
                        "address": {
                            "formatted": [
                                "Glasgow", 
                                "United Kingdom"
                            ], 
                            "components": {
                                "locality": [
                                    "Glasgow"
                                ], 
                                "country": "United Kingdom"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "Joachim.Gross@glasgow.ac.uk"
                ], 
                "contribution": "JG, Conception and design, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents", 
                "competingInterests": "The authors declare that no competing interests exist."
            }
        ], 
        "ethics": [
            {
                "type": "paragraph", 
                "text": "Human subjects: This study was approved by the local ethics committee (CSE01321; University of Glasgow, College of Science and Engineering) and conducted in conformity with the Declaration of Helsinki. All participants provided informed written consent before participating in the experiment and received monetary compensation for their participation."
            }
        ], 
        "funding": {
            "awards": [
                {
                    "id": "par-1", 
                    "source": {
                        "funderId": "10.13039/501100000268", 
                        "name": [
                            "Biotechnology and Biological Sciences Research Council"
                        ]
                    }, 
                    "awardId": "BB/L027534/1", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Christoph Kayser", 
                                "index": "Kayser, Christoph"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-2", 
                    "source": {
                        "funderId": "10.13039/501100000781", 
                        "name": [
                            "European Research Council"
                        ]
                    }, 
                    "awardId": "646657", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Christoph Kayser", 
                                "index": "Kayser, Christoph"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-3", 
                    "source": {
                        "funderId": "10.13039/100004440", 
                        "name": [
                            "Wellcome Trust"
                        ]
                    }, 
                    "awardId": "098433", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Joachim Gross", 
                                "index": "Gross, Joachim"
                            }
                        }
                    ]
                }, 
                {
                    "id": "par-4", 
                    "source": {
                        "funderId": "10.13039/100004440", 
                        "name": [
                            "Wellcome Trust"
                        ]
                    }, 
                    "awardId": "098434", 
                    "recipients": [
                        {
                            "type": "person", 
                            "name": {
                                "preferred": "Joachim Gross", 
                                "index": "Gross, Joachim"
                            }
                        }
                    ]
                }
            ], 
            "statement": "The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication."
        }, 
        "impactStatement": "Rhythms of lip movements entrain low-frequency brain oscillations and facilitate audiovisual speech processing.", 
        "keywords": [
            "lip movements", 
            "speech", 
            "language", 
            "oscillations", 
            "magnetoencephalography", 
            "electroencephalography"
        ], 
        "-related-articles-internal": [], 
        "-related-articles-external": [], 
        "digest": {
            "doi": "10.7554/eLife.14521.002", 
            "content": [
                {
                    "type": "paragraph", 
                    "text": "People are able communicate effectively with each other even in very noisy places where it is difficult to actually hear what others are saying. In a face-to-face conversation, people detect and respond to many physical cues \u2013 including body posture, facial expressions, head and eye movements and gestures \u2013 alongside the sound cues. Lip movements are particularly important and contain enough information to allow trained observers to understand speech even if they cannot hear the speech itself."
                }, 
                {
                    "type": "paragraph", 
                    "text": "It is known that brain waves in listeners are synchronized with the rhythms in a speech, especially the syllables. This is thought to establish a channel for communication \u2013 similar to tuning a radio to a certain frequency to listen to a certain radio station. Park et al. studied if listeners\u2019 brain waves also align to the speaker\u2019s lip movements during continuous speech and if this is important for understanding the speech."
                }, 
                {
                    "type": "paragraph", 
                    "text": "The experiments reveal that a part of the brain that processes visual information \u2013 called the visual cortex \u2013 produces brain waves that are synchronized to the rhythm of syllables in continuous speech. This synchronization was more precise in a complex situation where lip movements would be more important to understand speech. Park et al. also found that the area of the observer\u2019s brain that controls the lips (the motor cortex) also produced brain waves that were synchronized to lip movements. Volunteers whose motor cortex was more synchronized to the lip movements understood speech better. This supports the idea that brain areas that are used for producing speech are also important for understanding speech."
                }, 
                {
                    "type": "paragraph", 
                    "text": "Future challenges include understanding how synchronization of brain waves with the rhythms of speech helps us to understand speech, and how the brain waves produced by the visual and motor areas interact."
                }
            ]
        }, 
        "body": [
            {
                "type": "section", 
                "id": "s1", 
                "title": "Introduction", 
                "content": [
                    {
                        "type": "paragraph", 
                        "text": "Communication is one of the most fundamental and complex cognitive acts humans engage in. In a dialogue, a large range of dynamic signals are exchanged between interlocutors including body posture, (emotional) facial expressions, head and eye movements, gestures and a rich acoustic speech signal. Movements of the lips contain sufficient information to allow trained observers to comprehend speech through visual signals. Even for untrained observers and in the presence of auditory signals, lip movements can be beneficial for speech comprehension when the acoustic signal is degraded (<a href=\"#bib43\">Peelle and Sommers, 2015</a>; <a href=\"#bib52\">Sumby and Pollack, 1954</a>; <a href=\"#bib55\">van Wassenhove et al., 2005</a>; <a href=\"#bib63\">Zion-Golumbic and Schroeder, 2012</a>). Dynamic lip movements support disambiguation of syllables and can provide temporal onset cues for upcoming words or syllables (<a href=\"#bib10\">Chandrasekaran et al., 2009</a>; <a href=\"#bib18\">Grant and Seitz, 2000</a>; <a href=\"#bib25\">Kim and Davis, 2003</a>; <a href=\"#bib48\">Schroeder et al., 2008</a>; <a href=\"#bib58\">Schwartz and Savariaux, 2014</a>)."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "However, it has remained unclear how dynamic lip movements during continuous speech are represented in the brain and how these visual representations interact with the encoding of the acoustic speech signal. A potential underlying mechanism for the visual enhancement of hearing could be the synchronization of brain rhythms between interlocutors, which has been implicated in the encoding of acoustic speech (<a href=\"#bib17\">Giraud and Poeppel, 2012</a>; <a href=\"#bib22\">Hasson et al., 2012</a>; <a href=\"#bib44\">Pickering and Garrod, 2013</a>). Indeed, continuous speech and the associated lip movements show temporal modulations at the syllabic rate (3\u20138\u2009Hz) (<a href=\"#bib10\">Chandrasekaran et al., 2009</a>). These signals produced in the speaker\u2019s motor system supposedly lead to resonance in the listener\u2019s brain that facilitates speech comprehension (<a href=\"#bib17\">Giraud and Poeppel, 2012</a>). The hallmark of such a process is the synchronization of brain activity at the frequency of dominant rhythmic components in the communication signal (<a href=\"#bib48\">Schroeder et al., 2008</a>). Consistent with this idea, previous studies have demonstrated the frequency-specific synchronization between brain activity and continuous auditory speech signals (<a href=\"#bib1\">Ahissar et al., 2001</a>; <a href=\"#bib12\">Ding and Simon, 2012</a>; <a href=\"#bib20\">Gross et al., 2013b</a>; <a href=\"#bib29\">Luo and Poeppel, 2007</a>; <a href=\"#bib42\">Peelle et al., 2013</a>) at frequencies below 10\u2009Hz. This synchronization was found to be stronger for intelligible than non-intelligible speech and facilitated by top-down signals from left inferior frontal and motor areas (<a href=\"#bib13\">Ding and Simon, 2014</a>; <a href=\"#bib24\">Kayser et al., 2015</a>; <a href=\"#bib41\">Park et al., 2015</a>) as well as during attention (<a href=\"#bib63\">Zion-Golumbic and Schroeder, 2012</a>)."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "The correlated temporal dynamics of the acoustic and lip signals raise the possibility that lip-mediated benefits for hearing rely on similar entrainment mechanisms in the observer as the acoustic component. This is plausible as the auditory speech entrainment and speech intelligibility are enhanced when congruent visual speech is present (<a href=\"#bib11\">Crosse et al., 2015</a>; <a href=\"#bib62\">Zion Golumbic et al., 2013</a>). Still, the neural representations of dynamic lip signals and their dependence on attention and the acoustic speech component remain unclear."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "Here we directly tested four hypotheses: First, we hypothesized that rhythmic components in visual speech entrain brain activity in the observer. Second, to test whether benefits arising from seeing the speaker\u2019s lip movements are mediated through mechanism other than those implicated in auditory entrainment, we asked whether and which brain areas synchronize to lip movements independently of auditory signals. Third, we hypothesized that the synchronization between visual speech and brain activity is modulated by attention and congruence of visual and auditory signals. Finally, we tested whether any observed synchronization is relevant for speech comprehension."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "We recorded MEG signals while participants perceived continuous audiovisual speech. To dissociate the synchronization to attended and unattended visual and acoustic signals, we manipulated the congruency of visual and acoustic stimuli in four experimental conditions (<a href=\"#fig1\">Figure 1A</a>)."
                    }, 
                    {
                        "type": "image", 
                        "doi": "10.7554/eLife.14521.003", 
                        "id": "fig1", 
                        "label": "Figure 1.", 
                        "title": "Experimental conditions and behavioral results.", 
                        "caption": [
                            {
                                "type": "paragraph", 
                                "text": "(<b>A</b>) Four experimental conditions. \u2018A\u2019 denotes auditory stimulus and \u2018V\u2019 denotes visual stimulus.The number refers to the identity of each talk. All congruent condition: Natural audiovisual speech condition where auditory stimuli to both ears and visual stimuli are congruent (from the same movie; A1, A1, V1). All incongruent condition: All three stimuli are from different movies (A2, A3, V4) and participants are instructed to attend to auditory information presented to one ear. AV congruent condition: Auditory stimulus presented to one ear matches the visual information (A5, A6, V5). Participants attend to the talk that matches visual information. AV incongruent condition: Auditory stimulus presented to one ear matches the visual information (A7, A8, V8). Participants attend to the talk that does not match the visual information. Attended stimulus is marked as red color for the group attended to the left side (see Materials\u00a0and\u00a0methods for details).\u00a0(<b>B</b>) Behavioral accuracy by comprehension questionnaires. Congruent conditions show high accuracy rate compared to incongruent conditions (%; mean \u00b1 s.e.m.): All congruent: 85 \u00b1 1.66, All incongruent: 77.73 \u00b1 2.15, AV congruent: 83.40 \u00b1 1.73, AV incongruent: 75.68 \u00b1 2.88). Statistics between conditions show significant difference only between congruent and incongruent conditions (paired <i>t</i>-test, df: 43, p&lt;0.05)."
                            }
                        ], 
                        "alt": "", 
                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig1-v2.jpg", 
                        "filename": "elife-14521-fig1-v2.jpg"
                    }
                ]
            }, 
            {
                "type": "section", 
                "id": "s2", 
                "title": "Results", 
                "content": [
                    {
                        "type": "section", 
                        "id": "s2-1", 
                        "title": "Behavioral results", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "The four experimental conditions were designed to modulate congruence and informativeness of visual and auditory stimuli (<a href=\"#fig1\">Figure 1A</a>). In each condition, one visual stimulus was presented and two (identical or different) speech streams were presented to the left and the right ears, respectively (see Materials\u00a0and\u00a0methods for details). The <i>All congruent</i> condition consisted of three congruent stimuli. The <i>All incongruent</i> condition had three incongruent stimuli. In the <i>AV congruent</i> condition, participants attended an auditory stimulus that had a congruent visual stimulus with an additional incongruent auditory stimulus. In the <i>AV incongruent</i> condition participants attended an auditory stimulus that was incongruent to a congruent audiovisual stimulus pair."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Overall, participants showed high comprehension accuracy across conditions (%; mean \u00b1 s.e.m.): All congruent: 85 \u00b1 1.66, All incongruent: 77.73 \u00b1 2.15, AV congruent: 83.40 \u00b1 1.73, AV incongruent: 75.68 \u00b1 2.88). As expected, accuracy was significantly higher when the visual stimulus was congruent with attended auditory stimulus (i.e., All congruent and AV congruent conditions) compared to when the visual stimulus was incongruent with attended auditory stimulus (i.e., All incongruent and AV incongruent conditions) (<a href=\"#fig1\">Figure 1B</a>; paired <i>t</i>-test, df: 43, p&lt;0.05; All congruent vs. All incongruent: <i>t</i> = 3.09, p=0.003, All congruent vs. AV congruent: <i>t</i> = 0.76, p=0.45 (n.s.), All congruent vs. AV incongruent: <i>t</i> = 2.98, p=0.004, AV congruent vs. All incongruent: <i>t</i> = 2.15, p=0.03, AV congruent vs. AV incongruent: <i>t</i> = 2.24, p=0.03, All incongruent vs. AV incongruent: <i>t</i> = 0.65, p=0.52 (n.s.)). Interestingly, performance for AV congruent condition was not significantly different to performance in All congruent condition despite the interfering auditory input. This is likely caused by attentional efforts to overcome interfering input leading to behavioral compensation."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s2-2", 
                        "title": "Lip and sound signals are coherent during continuous speech", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "To examine the frequency spectrum of the lip signal, we computed the lip area for each video frame (<a href=\"#fig2\">Figure 2A</a> and <a href=\"#fig2s1\">Figure 2\u2014figure supplement 1A,B,C</a>). The signal is dominated by low-frequency components from 0 to 7\u2009Hz peaking around 0 to 4\u2009Hz (<a href=\"#fig2\">Figure 2B</a>; from all lip speech signals used in this study; mean \u00b1 s.e.m.). Next, we computed coherence between these lip signals and the respective acoustic signals to investigate the relationship between visual and auditory components in audiovisual speech. This was computed for all talks used in the study and averaged. The coherence spectrum reveals a prominent peak in a frequency band corresponding to the syllable rate around 4\u20138\u2009Hz (red line; mean \u00b1 s.e.m.) (<a href=\"#fig2\">Figure 2C</a>). These results demonstrate the temporal coupling of auditory and visual speech components."
                            }, 
                            {
                                "type": "image", 
                                "doi": "10.7554/eLife.14521.004", 
                                "id": "fig2", 
                                "label": "Figure 2.", 
                                "title": "Lip signals in continuous speech and its entrainment in the brain.", 
                                "caption": [
                                    {
                                        "type": "paragraph", 
                                        "text": "(<b>A</b>) Lip signals in the continuous audiovisual speech.Lip contour was extracted for each video frame and corresponding area was computed (see <a href=\"#fig2s1\">Figure 2\u2014figure supplement 1A,B,C</a> for details). One representative lip speech signal (for around 14\u00a0s speech) is shown here. Speaker\u2019s face is cropped for this publication only but not in the original stimuli.\u00a0(<b>B</b>) Spectral profile of lip speech signals. The power spectra of lip speech signals used in this study were averaged (mean \u00b1 s.e.m.). Signal is dominated by low-frequency components from 0 to 7 Hz that robustly peak around 0 to 4 Hz corresponding to delta and theta band neuronal oscillations in the brain.\u00a0(<b>C</b>) Coupling between lip and sound speech signals by coherence. Coherence between matching (red line) and non-matching (blue line) lip and sound speech signals were computed and averaged across talks used in the study (mean \u00b1 s.e.m.).\u00a0(<b>D</b>) Lip speech entrainment in natural audiovisual speech (All congruent condition). Coherence was computed between lip speech signal and brain activity at each voxel and then statistically compared to surrogate data at 1 Hz (the dominant frequency in the power spectrum in (<b>b</b>); p&lt;0.05, FDR corrected).\u00a0(<b>E</b>) Sound speech entrainment in natural audiovisual speech (All congruent condition). Using sound speech envelope, the same computation described in (<b>D</b>) was performed to investigate sound speech entrainment effect (p&lt;0.05, FDR corrected).\u00a0(<b>F</b>) Lip speech- and sound speech-specific entrainment effects. Lip speech (<b>D</b>) and sound speech coherence (<b>E</b>) were statistically compared (p&lt;0.05, FDR corrected)."
                                    }
                                ], 
                                "alt": "", 
                                "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig2-v2.jpg", 
                                "supplements": [
                                    {
                                        "type": "image", 
                                        "doi": "10.7554/eLife.14521.005", 
                                        "id": "fig2s1", 
                                        "label": "Figure 2\u2014figure supplement 1.", 
                                        "title": "A schematic figure for the analysis of coupling between lip movements and brain activity.", 
                                        "caption": [
                                            {
                                                "type": "paragraph", 
                                                "text": "We extracted lip movement signals by automatically computing for each video frame the lip contour using our in-house MATLAB script (<b>A</b>).\u00a0The contour was converted into the three quantities area, major axis and minor axis (<b>B</b>). We then computed the coherence between these measures (<b>C</b>). Area and minor axis (representing vertical lip movement) information have almost identical information approaching 1.0 in coherence value (red line; mean \u00b1 s.e.m.). However, major axis information representing horizontal lip movement only does not show similar pattern with either area or minor axis (purple and blue lines; mean \u00b1 s.e.m.). Since area information represents both horizontal and vertical lip movement, we used this information for further analysis. We then computed coherence between lip area signals and MEG signals at each voxel for each frequency band and for all four experimental conditions (<b>D</b>; see Materials\u00a0and\u00a0methods)."
                                            }
                                        ], 
                                        "alt": "", 
                                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig2-figsupp1-v2.jpg", 
                                        "filename": "elife-14521-fig2-figsupp1-v2.jpg"
                                    }, 
                                    {
                                        "type": "image", 
                                        "doi": "10.7554/eLife.14521.006", 
                                        "id": "fig2s2", 
                                        "label": "Figure 2\u2014figure supplement 2.", 
                                        "title": "Brain activity entrained by lip movements.", 
                                        "caption": [
                                            {
                                                "type": "paragraph", 
                                                "text": "We show lip-entrained brain activity by coherence in natural audiovisual speech condition (All congruent) at 1 Hz (dominant frequency component in lip signals) in <a href=\"#fig2\">Figure 2D</a>.\u00a0Here we show the same effect for other experimental conditions compared to surrogate data.\u00a0(<b>A</b>) AV congruent: Lip entrainment in the AV congruent condition showed similar pattern to All congruent condition (<a href=\"#fig2\">Figure 2D</a>). Distributed bilateral visual areas, auditory areas, left pre/postcentral gyri and right postcentral, supramarginal gyri were observed (p&lt;0.05, FDR corrected).\u00a0(<b>B</b>) All incongruent: Lip entrainment in this condition revealed increased involvement of bilateral visual cortex. This result confirms that there is clear lip entrainment effect irrespective of sound speech entrainment since this shows robust visual entrainment in the absence of a congruent sound speech input in All incongruent condition (p&lt;0.05, FDR corrected).\u00a0(<b>C</b>) AV incongruent: Compared to the other conditions lip entrainment is evident but reduced. Even though there is a congruent sound speech input as in the AV congruent condition, this does not reveal lip entrainment effect as strong as in the AV congruent condition providing more evidence that lip entrainment is modulated by attention (p &lt;0.05, FDR corrected)."
                                            }
                                        ], 
                                        "alt": "", 
                                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig2-figsupp2-v2.jpg", 
                                        "filename": "elife-14521-fig2-figsupp2-v2.jpg"
                                    }, 
                                    {
                                        "type": "image", 
                                        "doi": "10.7554/eLife.14521.007", 
                                        "id": "fig2s3", 
                                        "label": "Figure 2\u2014figure supplement 3.", 
                                        "title": "Brain activity entrained by lip movements and sound envelope.", 
                                        "caption": [
                                            {
                                                "type": "paragraph", 
                                                "text": "We show entrained brain activity by coherence separately for lip movements (<b>A</b>) and sound speech envelope (<b>B</b>) at each frequency from 2 to 5 Hz for All congruent condition in addition to 1 Hz shown in <a href=\"#fig2\">Figure 2D and E</a>."
                                            }
                                        ], 
                                        "alt": "", 
                                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig2-figsupp3-v2.jpg", 
                                        "filename": "elife-14521-fig2-figsupp3-v2.jpg"
                                    }
                                ], 
                                "filename": "elife-14521-fig2-v2.jpg"
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s2-3", 
                        "title": "Lip movements during continuous speech entrain brain activity", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "First, we tested the hypothesis that lip movements entrain the observer\u2019s brain activity. We addressed this by computing coherence between the lip signal and brain signal at each voxel at frequencies ranging from 1 to 7\u2009Hz (in 1\u2009Hz steps) covering the spectral profile of the lip signals (<a href=\"#fig2\">Figure 2B</a>). In addition, as a control, we computed surrogate maps (from time-shifted lip signals, thereby destroying physiologically meaningful coherence) as an estimate of spatially and spectrally specific biases of the analysis."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "We first compared natural audiovisual speech condition (All congruent) and surrogate data for the frequency that showed strongest power in the lip signal (1\u2009Hz). This revealed a significant entrainment effect in visual, auditory, and language areas bilaterally (p&lt;0.05, false discovery rate (FDR) corrected; <a href=\"#fig2\">Figure 2D</a>). The areas include early visual (V1; Calcarine sulcus) and auditory (A1; Heschl\u2019s gyrus) areas as well as inferior frontal gyrus (IFG; BA 44) (see <a href=\"#fig2s2\">Figure 2\u2014figure supplement 2</a> for the other conditions at 1\u2009Hz)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "However, since the speech envelope and lip movements are coherent (<a href=\"#fig2\">Figure 2C</a>), it may be that this lip entrainment is induced by speech entrainment and not by lip movements per se. Thus, we performed the same coherence analysis for the sound speech envelope. In accordance with previous work (<a href=\"#bib20\">Gross et al., 2013b</a>), we observed an extensive auditory network including Heschl\u2019s gyrus and superior/middle temporal gyri bilaterally and left frontal areas (p&lt;0.05, FDR corrected; <a href=\"#fig2\">Figure 2E</a>)(see <a href=\"#fig2s3\">Figure 2\u2014figure supplement 3</a> for different frequencies [2\u20135\u2009Hz]). Statistical comparison of lip movement entrainment (<a href=\"#fig2\">Figure 2D</a>) to sound speech entrainment (<a href=\"#fig2\">Figure 2E</a>) revealed significantly stronger lip entrainment in bilateral visual areas and stronger sound speech coherence in right superior temporal gyrus (p&lt;0.05, FDR corrected; <a href=\"#fig2\">Figure 2F</a>). This demonstrates significant entrainment of brain activity to the lip movements irrespective of entrainment to the acoustic speech signal. In addition, we found significant lip movement entrainment in visual areas in the absence of a congruent auditory stimulus (<a href=\"#fig2s2\">Figure 2\u2014figure supplement 2B</a>). These results demonstrate for the first time the entrainment of cortical brain oscillations to lip movements during continuous speech."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s2-4", 
                        "title": "Lip entrainment is modulated by attention and congruence", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Next, we compared visual lip entrainment across conditions to test our hypothesis that entrainment changes with attention and the congruence of audiovisual stimuli. We focused our analysis on AV congruent condition where a distracting auditory speech stream is presented to one ear. Compared to the All congruent condition, AV congruent demands additional attention to visual speech because the visual signal is informative to disambiguate the two incongruent auditory streams. We therefore contrasted AV congruent with All congruent condition to capture the effect of visual attention. We also contrasted AV congruent condition with All incongruent condition to capture the effect of congruence."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Since lip and sound speech signals are coherent (<a href=\"#fig2\">Figure 2C</a>), it is difficult to disentangle visual and auditory contributions to the lip movement entrainment. To measure lip-specific entrainment effects more directly, we computed partial coherence between lip movement signals and brain activity while removing the contribution of acoustic speech signals. This provides an estimate of entrainment by lip movement signals that cannot be explained by acoustic speech signals and allowed us to test our second hypothesis that lip entrainment is not mediated via acoustic entrainment. However, we repeated the same analysis using coherence instead of partial coherence (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1</a>)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "First, we identified the frequency band showing the strongest attention effect by averaging voxels across visual cortex (superior/middle/inferior occipital gyri) defined from AAL (Automated Anatomical Labeling) ROI (Region-of-Interest) map. We averaged the partial coherence values within the ROI and statistically compared the AV congruent to surrogate data and to All congruent condition at each frequency from 1 to 7\u2009Hz (<a href=\"#fig3\">Figure 3A</a>; paired <i>t</i>-test, df: 43, red dashed line: p&lt;0.05, gray dashed line: p&lt;0.05, corrected). This revealed significantly stronger lip movement entrainment at 4\u2009Hz in the left visual cortex for AV congruent compared to both, All congruent condition and surrogate data. In AV congruent condition, lip movements are informative and assist comprehension. This result suggests that coupling of low-frequency brain activity to lip movements is enhanced by visual attention."
                            }, 
                            {
                                "type": "image", 
                                "doi": "10.7554/eLife.14521.008", 
                                "id": "fig3", 
                                "label": "Figure 3.", 
                                "title": "Lip-brain partial coherence.", 
                                "caption": [
                                    {
                                        "type": "paragraph", 
                                        "text": "(<b>A</b>) Modulation of partial lip-brain coherence by attention and congruence in visual ROI.AV congruent condition was compared to the other conditions (paired <i>t</i>-test, df: 43, red dashed line: p&lt;0.05, gray dashed line: p&lt;0.05, corrected).\u00a0(<b>B</b>, <b>C</b>, <b>D</b>) Attention-modulated partial coherence at each brain voxel (AV congruent versus surrogate (<b>B</b>), All incongruent (<b>C</b>), All congruent (<b>D</b>)). It shows significant involvement of left motor cortex (precentral gyrus; BA 4/6) and left visual areas (p&lt;0.05, FDR corrected; but in (<b>D</b>), left motor cortex is observed at uncorrected p&lt;0.05). Entrainment in the left motor cortex shows a systematic modulation such that statistical contrast with a strong difference in visual attention show stronger entrainment (AV congruent versus surrogate (<b>B</b>; <i>t</i><sub>43</sub>-value: 3.42) &gt; All incongruent (<b>C</b>; <i>t</i><sub>43</sub>-value: 3.20) &gt; All congruent (<b>D</b>; <i>t</i><sub>43</sub>-value: 2.24))."
                                    }
                                ], 
                                "alt": "", 
                                "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig3-v2.jpg", 
                                "supplements": [
                                    {
                                        "type": "image", 
                                        "doi": "10.7554/eLife.14521.009", 
                                        "id": "fig3s1", 
                                        "label": "Figure 3\u2014figure supplement 1.", 
                                        "title": "Lip-brain coherence.", 
                                        "caption": [
                                            {
                                                "type": "paragraph", 
                                                "text": "(<b>A</b>, <b>B</b>) Modulation of lip entrainment by attention and congruence in ROIs.Contrasting AV congruent with All congruent for the effect of attention and with All incongruent for the effect of congruence are shown in left visual (superior/middle/inferior occipital gyri) and right auditory (Heschl\u2019s gyrus) areas defined by AAL ROI maps (paired <i>t</i>-test, df: 43, red dashed line: p&lt;0.05, gray dashed line: p&lt;0.05, corrected) at each frequency from 1 to 7 Hz. The strongest difference was observed at 4 Hz for both contrasts in both areas.(<b>C</b>, <b>D</b>) Spatial distribution of lip entrainment modulation by attention and congruence at 4 Hz (p&lt;0.05, FDR corrected). Significant effects are observed in left visual areas, right superior and middle temporal gyri including Heschl\u2019s gyrus, posterior superior temporal sulcus (pSTS), left precentral gyrus and right inferior frontal gyrus (IFG; BA 44/6)."
                                            }
                                        ], 
                                        "alt": "", 
                                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig3-figsupp1-v2.jpg", 
                                        "filename": "elife-14521-fig3-figsupp1-v2.jpg"
                                    }, 
                                    {
                                        "type": "image", 
                                        "doi": "10.7554/eLife.14521.010", 
                                        "id": "fig3s2", 
                                        "label": "Figure 3\u2014figure supplement 2.", 
                                        "title": "Partial coherence between lip movements and left motor cortex.", 
                                        "caption": [
                                            {
                                                "type": "paragraph", 
                                                "text": "We here show a frequency-specific plot for partial coherence between lip movements and left motor cortex.We extracted the maximum voxel in the left motor cortex (precentral gyrus) from the contrast for attention effect (AV congruent vs. All congruent shown in <a href=\"#fig3\">Figure 3D</a>; MNI coordinates = [-44 -16 56]). This confirmed lip movements entrain left motor cortex at 4 Hz irrespective of auditory speech signals."
                                            }
                                        ], 
                                        "alt": "", 
                                        "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig3-figsupp2-v2.jpg", 
                                        "filename": "elife-14521-fig3-figsupp2-v2.jpg"
                                    }
                                ], 
                                "filename": "elife-14521-fig3-v2.jpg"
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Next, we studied attentional lip movement entrainment at 4\u2009Hz across the entire brain. This revealed a significant partial coherence between lip movements and left motor cortex (precentral gyrus; BA 4/6) in addition to the left visual areas (p&lt;0.05, FDR corrected; <a href=\"#fig3\">Figure 3B,C,D</a>). Entrainment in the left motor cortex shows a systematic modulation by attention. Specifically, contrasts with a stronger difference in visual informativeness exhibit stronger lip entrainment (AV congruent versus surrogate (<a href=\"#fig3\">Figure 3B</a>; <i>t</i><sub>43</sub>-value: 3.42) &gt; All incongruent (<a href=\"#fig3\">Figure 3C</a>; <i>t</i><sub>43</sub>-value: 3.20) &gt; All congruent (<a href=\"#fig3\">Figure 3D</a>; <i>t</i><sub>43</sub>-value: 2.24)). Contrasting AV congruent with All congruent conditions (<a href=\"#fig3\">Figure 3D</a>), which both have congruent visual speech, also revealed an effect in the same motor cortex area (p&lt;0.05, uncorrected; for the frequency-specific plot for left motor cortex, see <a href=\"#fig3s2\">Figure 3\u2014figure supplement 2</a>). In the addition, the left visual cortex shows significantly stronger lip entrainment for AV congruent compared to surrogate data (<a href=\"#fig3\">Figure 3B</a>), All incongruent condition (<a href=\"#fig3\">Figure 3C</a>) and All congruent condition (<a href=\"#fig3\">Figure 3D</a>). This demonstrates that activity in left motor cortex and left visual areas show significant alignment to lip movements independent of sound speech signals and that this alignment is stronger when visual speech is more informative and congruent to the auditory stimulus."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s2-5", 
                        "title": "Attentional enhancement of lip entrainment in left motor cortex facilitates speech comprehension", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "To address our fourth hypothesis that lip-entrained brain activity has an impact on speech comprehension, we identified brain regions where lip entrainment correlates with behavioral performance. We performed regression analysis using comprehension accuracy across subjects on the partial coherence map at 4\u2009Hz in each condition. We then contrasted the condition with high visual attention and behavioral performance (AV congruent) to the condition with low visual attention and behavioral performance (AV incongruent; see behavioral results in <a href=\"#fig1\">Figure 1B</a>). The regression <i>t</i>-values in the two conditions were transformed to standardized <i>Z</i>-value at each voxel and the two <i>Z</i>-maps were subtracted. This revealed that entrainment in left motor cortex predicts attention-modulated comprehension accuracy (<a href=\"#fig4\">Figure 4A</a>; <i>Z</i>-difference map at p&lt;0.005)."
                            }, 
                            {
                                "type": "image", 
                                "doi": "10.7554/eLife.14521.011", 
                                "id": "fig4", 
                                "label": "Figure 4.", 
                                "title": "Behavioral correlates of attentional lip entrainment.", 
                                "caption": [
                                    {
                                        "type": "paragraph", 
                                        "text": "(<b>A</b>) Lip-entrained brain regions predicted by attention-modulated comprehension accuracy.Regression analysis using comprehension accuracy across participants on the partial coherence map was performed at 4 Hz in each condition. Then <i>Z</i>-difference map was obtained from the regression analysis of conditions showing strongest difference in comprehension accuracy (AV congruent versus AV incongruent; see behavioral results in <a href=\"#fig1\">Figure 1B</a>). This revealed that the left motor cortex entrainment predicts attention-modulated comprehension accuracy (<i>Z</i>-difference map at p&lt;0.005).\u00a0(<b>B</b>) Correlation between partial coherence in left motor cortex and comprehension accuracy. Partial coherence values from the maximum coordinate in the left motor cortex and comprehension accuracy across subjects for the AV congruent condition was positively correlated (Pearson\u2019s coefficient of Fisher\u2019s <i>Z</i>-transformed data <i>R</i> = 0.38, <i>P</i> = 0.01; Spearman rank correlation <i>R</i> = 0.32, <i>P</i> = 0.03).\u00a0(<b>C</b>) Difference of attentional lip-entrainment in left motor cortex between good and poor performing group. Group <i>t</i>-statistics between good and poor performing group on the extracted partial coherence values in the left motor cortex for the AV congruent condition was performed. The two groups were divided using median value (90%; 23 good versus 21 poor performers) of comprehension accuracy for the AV congruent condition. Good performers showed higher partial coherence value in left motor cortex than poor performers (two-sample <i>t</i>-test on Fisher\u2019s <i>Z</i>-transformed data; <i>t</i><sub>42</sub> = 2.2, <i>P</i> = 0.03)."
                                    }
                                ], 
                                "alt": "", 
                                "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-fig4-v2.jpg", 
                                "filename": "elife-14521-fig4-v2.jpg"
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "To confirm this effect, we correlated partial coherence values from the maximum coordinate in the left motor cortex with comprehension accuracy across subjects for the AV congruent condition. This revealed significant positive correlation demonstrating that individuals with higher levels of comprehension show higher partial coherence in the left motor cortex. Since the accuracy has discrete values such as 70, 80, 90% (correct response rate out of 10 questions at the comprehension testing), we plotted the correlation as a box plot (<a href=\"#fig4\">Figure 4B</a>; Pearson\u2019s coefficient of Fisher\u2019s <i>Z</i>-transformed data <i>R</i> = 0.38, <i>P</i> = 0.01; Spearman rank correlation <i>R</i> = 0.32, <i>P</i> = 0.03)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "To confirm this result in a separate analysis, we performed <i>t</i>-statistics between good and poor performing group on the extracted partial coherence values in the AV congruent condition. The two groups were divided using median value of performance leading to 23 good performers and 21 poor performers. Good performers showed significantly higher partial coherence value in the left motor cortex than poor performers (<a href=\"#fig4\">Figure 4C</a>; two-sample <i>t</i>-test on Fisher\u2019s <i>Z</i>-transformed data; <i>t</i><sub>42</sub> = 2.2, <i>P</i> = 0.03). Taken together, these results demonstrate that stronger attentional lip entrainment in the left motor cortex supports better speech comprehension."
                            }
                        ]
                    }
                ]
            }, 
            {
                "type": "section", 
                "id": "s3", 
                "title": "Discussion", 
                "content": [
                    {
                        "type": "paragraph", 
                        "text": "Here we provide the first direct evidence that lip movements during continuous speech entrain low-frequency brain oscillations in speech processing brain areas, that this entrainment is modulated by attention and congruence, and that entrainment in motor regions correlates with speech comprehension."
                    }, 
                    {
                        "type": "section", 
                        "id": "s3-1", 
                        "title": "Speaking lips entrain low-frequency oscillations during natural audiovisual speech", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Recent studies provide converging evidence that detailed information about the identity or specific features of sensory stimuli can be decoded from the low-frequency phase of LFP or MEG/EEG signals (<a href=\"#bib34\">Ng et al., 2013</a>; <a href=\"#bib40\">Panzeri et al., 2015</a>; <a href=\"#bib49\">Schyns et al., 2011</a>). For example, a recent study has demonstrated that the identity of different audiovisual movie stimuli can be decoded from delta-theta phase in occipital MEG sensors (<a href=\"#bib28\">Luo et al., 2010</a>). In the auditory domain, this stimulus-specificity is at least partly due to a phase synchronization of rhythmic brain activity and the auditory speech envelope (<a href=\"#bib20\">Gross et al., 2013b</a>; <a href=\"#bib42\">Peelle et al., 2013</a>). Recent studies have shown that congruent visual stimulation facilitates auditory speech entrainment (<a href=\"#bib11\">Crosse et al., 2015</a>; <a href=\"#bib62\">Zion Golumbic et al., 2013</a>). Here, we extend these results by showing low-frequency phase synchronization between speakers\u2019 lip movements and listeners\u2019 brain activity. This visual entrainment is clearly distinct from auditory entrainment for three reasons. First, some areas show stronger entrainment to the visual compared than to the auditory stimulus. Second, removing the auditory contribution using partial coherence still results in significant visual entrainment. Third, we report visual entrainment in the absence of a congruent auditory stimulus. Together, this establishes the existence of a visual entrainment mechanism for audiovisual speech in addition to the well-studied auditory entrainment. In correspondence to the auditory domain, this effect relies on the quasi-rhythmic nature of visual speech that is particularly prominent in the subtle but salient lip movements."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Our coherence analysis reveals an extensive network comprising speech processing areas such as bilateral primary sensory areas (V1 (Calcarine sulcus; BA 17) and A1 [Heschl\u2019s gyrus; BA 41]), extended sensory visual (BA 18/19) and auditory areas (superior/middle temporal gyri; BA 21/22/42), and posterior superior temporal sulcus (pSTS) as well as inferior frontal gyrus (IFG; BA 44/6). Although these areas show synchronization to lip movements, there is large overlap with areas showing synchronization to sound envelope. This is not surprising because both stimulus signals are coherent. But as expected, in occipital areas visual entrainment is significantly stronger compared to auditory entrainment."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "An interesting asymmetry emerges in auditory areas in the comparison of lip entrainment to speech entrainment (<a href=\"#fig2\">Figure 2F</a>). The right superior temporal cortex is significantly stronger coupled to speech envelope than to lip movements while there is no significant difference in left superior temporal cortex. This seems to suggest that visual speech predominantly entrains left temporal areas (in addition to visual areas). This is consistent with a recent fMRI study demonstrating preferential processing of visual speech in left superior temporal areas (<a href=\"#bib6\">Blank and von Kriegstein, 2013</a>)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In summary, we find that visual areas are entrained by lip movements and left and right auditory areas by lip movements and speech envelope with only the right auditory cortex showing a stronger coupling to speech envelope compared to lip movements."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s3-2", 
                        "title": "Attention and congruence modulate audiovisual entrainment", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "We studied the effect of attention in an ecologically valid scenario with congruent audiovisual speech when a distracting auditory stimulus was present (AV congruent condition) or absent (All congruent condition). While we did not explicitly manipulate attention in our paradigm, the distracting auditory stimulus in the AV congruent condition renders the visual speech relevant and informative. Attention to visual speech is known to help to disambiguate the competing auditory inputs (<a href=\"#bib52\">Sumby and Pollack, 1954</a>; <a href=\"#bib62\">Zion Golumbic et al., 2013</a>). This condition was compared to the All congruent condition where attention to the visual stimulus is not required for speech comprehension."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Interestingly, attention to lip movements leads to significantly increased coupling in left hemispheric visual areas. Previous studies have demonstrated that observers process audiovisual speech preferentially based on information from the left side of a speaker\u2019s face (observers\u2019 right visual hemifield) as compared to the right side (<a href=\"#bib4\">Behne et al., 2008</a>; <a href=\"#bib9\">Campbell et al., 1996</a>; <a href=\"#bib50\">Smeele et al., 1998</a>; <a href=\"#bib53\">Swerts and Krahmer, 2008</a>) (but see <a href=\"#bib35\">Nicholls and Searle, 2006</a>). This has been related to an attentional bias to the observers\u2019 right visual hemifield due to the left hemisphere dominance for speech processing (<a href=\"#bib54\">Thompson et al., 2004</a>). Since the right visual hemifield is represented in the left visual cortex, this attentional bias could explain the observed lateralization of entrainment."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "While visual areas show both coherence and partial coherence to lip movements, temporal areas only emerge from coherence analysis (<a href=\"#fig2\">Figure 2D</a> and <a href=\"#fig3s1\">Figure 3\u2014figure supplement 1</a>). This seems to suggest that the temporal alignment of auditory cortex activity to lip movements is not independant of the congruent acoustic speech stimulus and therefore speaks against a direct entrainment of auditory cortex activity by lip movements. However, the informativeness of lip movements still modulates significantly the coherence between lip movements and right temporal brain areas (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1B,C,D</a>) and indicates an indirect effect of visual attention. Similarly, left posterior superior temporal sulcus (pSTS) shows a significant effect of congruence (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1D</a>) that is only evident in coherence and not in partial coherence map. This is consistent with previous reports of pSTS as a locus of multisensory integration (<a href=\"#bib23\">Hocking and Price, 2008</a>; <a href=\"#bib36\">Noesselt et al., 2012</a>) with particular relevance for visual speech recognition (<a href=\"#bib3\">Beauchamp et al., 2004</a>; <a href=\"#bib6\">Blank and von Kriegstein, 2013</a>; <a href=\"#bib57\">Werner and Noppeney, 2010</a>)."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s3-3", 
                        "title": "Motor areas are entrained by speaking lips", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Attended audiovisual speech leads to significant entrainment in left inferior precentral gyrus corresponding to the lip representation in motor cortex (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1C,D</a>) (<a href=\"#bib16\">Giraud et al., 2007</a>). In addition, partial coherence analysis where the effect of speech envelope on lip entrainment is removed reveals a more superior left lateralized motor area (<a href=\"#fig3\">Figure 3B,C,D</a>). Partial coherence in this area is modulated by congruence (<a href=\"#fig3\">Figure 3C</a>) and attention (<a href=\"#fig3\">Figure 3D</a>) and predicts comprehension accuracy (<a href=\"#fig4\">Figure 4A</a>). There is ample evidence for the activation of left motor cortex during audiovisual speech perception (<a href=\"#bib14\">Evans and Davis, 2015</a>; <a href=\"#bib30\">Meister et al., 2007</a>; <a href=\"#bib33\">Mottonen et al., 2013</a>; <a href=\"#bib56\">Watkins et al., 2003</a>; <a href=\"#bib60\">Wilson et al., 2004</a>; <a href=\"#bib61\">Ylinen et al., 2015</a>). Our results indicate that left motor areas are not only activated but also entrained by audiovisual speech. This establishes a precise temporal coupling between audiovisual sensory inputs and sensory and motor areas that could temporally coordinate neuronal computations associated with speech processing. As part of a proposed dorsal auditory pathway, motor areas could provide access to an internal forward model of speech that closely interacts with auditory sensory systems (<a href=\"#bib7\">Bornkessel-Schlesewsky et al., 2015</a>; <a href=\"#bib45\">Rauschecker and Scott, 2009</a>). During speech production, an efference copy is sent to auditory areas to allow for efficient monitoring and control. During speech perception, sensory signals could be used to constrain the forward model in simulating the speaker\u2019s motor program and predicting upcoming sensory events (<a href=\"#bib2\">Arnal and Giraud, 2012</a>; <a href=\"#bib7\">Bornkessel-Schlesewsky et al., 2015</a>; <a href=\"#bib26\">Lakatos et al., 2013</a>)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Indeed, a recent study has demonstrated direct top-down control of left auditory cortex from left motor cortex during continuous speech processing. The strength of low-frequency top-down signals in the left motor cortex correlates with the coupling of auditory cortex and sound speech envelope (<a href=\"#bib41\">Park et al., 2015</a>) (see also <a href=\"#bib24\">Kayser et al., 2015</a>). This suggests that the motor cortex plays a predictive role in speech perception consistent with recent demonstrations of its contribution to the temporal precision of auditory speech perception (<a href=\"#bib32\">Morillon et al., 2014</a>; <a href=\"#bib31\">2015</a>; <a href=\"#bib59\">Wilson et al., 2008</a>)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Visual speech <i>per se</i> is not critical for speech comprehension, however, it facilitates auditory speech processing as it aids temporal prediction and can prime the auditory system for upcoming concordant auditory input (<a href=\"#bib43\">Peelle and Sommers, 2015</a>; <a href=\"#bib55\">van Wassenhove et al., 2005</a>). Overall, left motor cortex seems to be an important area for facilitating audiovisual speech processing through predictive control in auditory active sensing (<a href=\"#bib30\">Meister et al., 2007</a>; <a href=\"#bib31\">Morillon et al., 2015</a>)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In summary, our study provides the first direct evidence that lip movements during continuous speech entrain visual and motor areas, and that this entrainment is modulated by attention and congruence and is relevant for speech comprehension. This adds to similar findings in the auditory domain and provides a more comprehensive view of how temporally correlated auditory and visual speech signals are processed in the listener\u2019s brain. Overall, our findings support an emerging model where rhythmic audiovisual signals entrain multisensory brain areas and dynamically interact with an internal forward model accessed via the auditory dorsal stream to form dynamically updated predictions that improve further sensory processing. Through these mechanisms brain oscillations might implement inter-subject synchronization and support the surprising efficiency of inter-human communication."
                            }
                        ]
                    }
                ]
            }, 
            {
                "type": "section", 
                "id": "s4", 
                "title": "Materials and methods", 
                "content": [
                    {
                        "type": "section", 
                        "id": "s4-1", 
                        "title": "Participants", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Data were obtained from 46 healthy subjects (26 females; age range: 18\u201330 years; mean age: 20.54 \u00b1 2.58 years) and they were all right-handed confirmed by Edinburgh Handedness Inventory (<a href=\"#bib38\">Oldfield, 1971</a>). All participants provided informed written consent before participating in the experiment and received monetary compensation for their participation. All participants had normal or corrected-to-normal vision and normal hearing. None of the participants had a history of developmental, psychological, or neurological disorders. Only native English-speaking volunteers with British nationality were recruited due to the British accent in the stimulus material. Two subjects were excluded from the analysis (one subject fell asleep and one had MEG signals with excessive noise). This left dataset from 44 participants (25 females; age range: 18\u201330 years; mean age: 20.45 \u00b1 2.55 years). This study was approved by the local ethics committee (CSE01321; University of Glasgow, College of Science and Engineering) and conducted in conformity with the Declaration of Helsinki."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-2", 
                        "title": "Data acquisition", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Neuromagnetic signals were obtained with a 248-magnetometers whole-head MEG (Magnetoencephalography) system (MAGNES 3600 WH, 4-D Neuroimaging) in a magnetically shielded room using a sampling rate of 1017\u2009Hz. MEG signals were denoised with information from the reference sensors using the denoise_pca function in FieldTrip toolbox (<a href=\"#bib39\">Oostenveld et al., 2011</a>). Bad sensors were excluded by visual inspection. Electrooculographic (EOG) and electrocardiographic (ECG) artifacts were eliminated using independent component analysis (ICA). Participants\u2019 eye fixation and movements were recorded during the experiment using an eye tracker (EyeLink 1000, SR Research Ltd.) to ensure that they fixate on the speaker\u2019s lip."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "T1-weighted structural magnetic resonance images (MRI) were acquired at 3\u2009T Siemens Trio Tim scanner (Siemens, Erlangen, Germany) with the following parameters: 1.0 x 1.0 x 1.0 mm<sup>3</sup> voxels; 192 sagittal slices; Field of view (FOV): 256 x 256 matrix.\u00a0Data will be available upon request by contacting corresponding authors."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-3", 
                        "title": "Stimuli and Experiment", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "The stimuli used in this study were audiovisual video clips of a professional male speaker talking continuously (7\u20139 min). The talks were originally taken from TED talks (www.ted.com/talks/) and edited to be appropriate to the stimuli we used (e.g. editing words referring to visual materials, the gender of the speaker)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Eleven video clips were filmed by a professional filming company with high-quality audiovisual device and recorded in 1920 x 1080 pixels at 25 fps (frame per second) for video and sampling rate of 48 kHz for audio."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In a behavioral study, these videos were rated by 33 participants (19 females; aged 18\u201331 years; mean age: 22.27 \u00b1 2.64 years) in terms of <i>arousal, familiarity, valence, complexity, significance (informativeness), agreement (persuasiveness), concreteness, self-relatedness, and level of understanding</i>. Participants were instructed to rate each talk on these 9 items using Likert scale (<a href=\"#bib27\">Likert, 1932</a>) 1 to 5 (for an example of concreteness, 1: very abstract, 2: abstract, 3: neither abstract nor concrete, 4: concrete, 5: very concrete). Talks with excessive mean scores (below 1 and over 4) were excluded and 8 talks were selected for the experiment."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "There were four experimental conditions: All congruent, All incongruent, AV congruent, AV incongruent (<a href=\"#fig1\">Figure 1A</a>). In each condition, one video recording was presented and two (identical or different) auditory recordings were presented to the left and the right ear, respectively."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "All congruent condition. Natural audiovisual speech condition where auditory stimuli to both ears and visual stimuli are congruent (from the same movie; e.g. A1, A1, V1 \u2013 where the first A denotes stimulus presented to the left ear, second A denotes stimulus presented to right ear and V denotes visual stimulus. The number refers to the identity of each talk.)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "All incongruent condition. All three stimuli are from different movies (e.g. A2, A3, V4) and participants are instructed to attend to auditory information presented to one ear."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "AV congruent condition. Auditory stimulus presented to one ear matches the visual information (e.g. A5, A6, V5). Participants attend to the talk that matches visual information."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "AV incongruent condition. Auditory stimulus presented to one ear matches the visual information (e.g. A7, A8, V8). Participants attend to the talk that does not match the visual information."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "The color (yellow or blue) of a small fixation cross which was overlaid on the speaker\u2019s lip indicates the side of attention (left or right talk, e.g. \u201cIf the color of fixation cross is yellow, please attend to left ear talk.\u201d). The functional meaning of the color of fixation cross was counterbalanced across subjects. In All congruent condition (natural audiovisual speech), participants were instructed to ignore the color and just to attend to both sides. Participants were instructed to fixate on the speaker\u2019s lip all the time in all experimental conditions even if they found it difficult to do so in some conditions (e.g. incongruent conditions; All incongruent and AV incongruent). The fixation cross was used to guide participants\u2019 fixation on the speaker\u2019s lip. Furthermore, the gaze behavior was monitored by an eye tracker. The importance of eye fixation on the speaker\u2019s lip was stressed at the task instruction and they were notified that their eye fixation would be monitored by an eye tracker."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "There were two groups (22 subjects each). Participants in one group attended to left ear talk and participants in another group attended to right ear talk in the experiment (for all four conditions). Since the attended side is the same within subjects, in order to avoid presenting all the same color of fixation cross (e.g. yellow) within subjects and to prevent them from sensing that they always attend to one side (left or right), the All congruent condition was always presented in the middle (second or third) among the four conditions using color of fixation cross indicating opposite side (e.g. blue). As expected there was no significant difference in comprehension accuracy between groups (two sample <i>t</i>-test, df: 42, p&gt;0.05) and for the questions addressed here data was pooled across both groups."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In order to prevent talk-specific effects, we used two sets of stimuli consisting of different talks in the combination of audiovisual talks and these two sets were randomized across subjects (each set 1 and 2 was used for a half of subjects (22 subjects)). For example, talks used for AV incongruent condition in the set 1 were used for All incongruent condition in the set 2."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "To assess the level of comprehension, we designed questionnaires for each talk. Each questionnaire consists of 10 questions about the talk and tests general comprehension of the talk (e.g., \u201cWhat is the speaker\u2019s job?\u201d). These questionnaires were also validated from another set of participants (16 subjects; 13 females; aged 18\u201323 years; mean age: 19.88 \u00b1 1.71 years) to ensure the same level of difficulty (accuracy) across questionnaires for the talks and the length (word count) of the questionnaires also matched across all the questionnaires. The attended 4 talks (in the 4 conditions) were counterbalanced across conditions in the two sets, thus comprehension of 4 talks were validated. Three participants who showed poor performance (below 60% accuracy) were excluded from the analysis. There were no significant differences in the comprehension accuracy between the talks (mean \u00b1 s.e.m. accuracy (%) for talk 1: 84.62 \u00b1 2.91; talk 2: 87.69 \u00b1 2.31; talk 3: 90.77 \u00b1 2.39; talk 4: 85.38 \u00b1 2.43; p&gt;0.05 at all pair-wise <i>t</i>-tests between talks)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In order to recombine audiovisual talks for the four experimental conditions and to add fixation cross, we used Final Cut Pro X (Apple Inc., Cupertino, CA)."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "The stimuli were controlled with Psychtoolbox (<a href=\"#bib8\">Brainard, 1997</a>) under MATLAB (MathWorks, Natick, MA). Visual stimuli were delivered with a resolution of 1280 x 720 pixels at 25 fps (mp4 format). Auditory stimuli were delivered at 48 kHz sampling rate via a sound pressure transducer through two 5 meter-long plastic tubes terminating in plastic insert earpieces."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "Each condition (continuous audiovisual speech) lasted 7\u20139 min. After each condition, comprehension questionnaire was performed about the attended talk. Here we measured both accuracy and response time and participants were asked to respond as accurately and quickly as possible. After the experiment, post-experimental questionnaire was administered to obtain participants\u2019 feedback about the experiment."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-4", 
                        "title": "Data analysis", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "The analysis of MEG data was performed using the FieldTrip toolbox (<a href=\"#bib39\">Oostenveld et al., 2011</a>) and in-house MATLAB codes according to guidelines (<a href=\"#bib19\">Gross et al., 2013a</a>)."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-5", 
                        "title": "Lip speech (visual) signal processing", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "We used in-house Matlab code to extract lip contour of the speaker for each frame of each movie (<a href=\"#fig2\">Figure 2A</a>). From the lip contour we computed area information (area within lip contour), major axis information (horizontal axis within lip contour) and minor axis information (vertical axis within lip contour). For our analysis we used area information of lip contour (see <a href=\"#fig2s1\">Figure 2\u2014figure supplement 1A,B,C</a> for details) although use of vertical axis leads to qualitatively similar results. This signal was resampled at 250\u2009Hz to match the sampling rate of the preprocessed MEG signal."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-6", 
                        "title": "Sound speech (auditory) signal processing", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "We computed the amplitude envelope of sound speech signals (<a href=\"#bib10\">Chandrasekaran et al., 2009</a>). We constructed eight frequency bands in the range 100\u201310,000\u2009Hz to be equidistant on the cochlear map (<a href=\"#bib51\">Smith et al., 2002</a>). Then sound speech signals were band-pass filtered in these bands using a fourth-order Butterworth filter (forward and reverse). Hilbert transform was applied to obtain amplitude envelopes for each band. These signals were then averaged across bands and resulted in a wideband amplitude envelope. Finally, these signals were downsampled to 250\u2009Hz for further analysis."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-7", 
                        "title": "MEG-MRI co-registration", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "MR image of each participant was co-registered to the MEG coordinate system using a semi-automatic procedure. Anatomical landmarks such as nasion, bilateral pre-auricular points were manually identified in the individual\u2019s MRI. Based on these three points, both coordinate systems were initially aligned. Subsequently, numerical optimization was achieved by using the ICP algorithm (<a href=\"#bib5\">Besl and McKay, 1992</a>)."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-8", 
                        "title": "Source localization", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Individual head model was created from structural MRI using segmentation routines in FieldTrip and SPM8. Leadfield computation was based on a single shell volume conductor model (<a href=\"#bib37\">Nolte, 2003</a>) using a 8-mm grid defined on the MNI (Montreal Neurological Institute) template brain. For spatial normalization to the standard template, the template grid was transformed into individual head space by linear spatial transformation. Cross-spectral density matrices were computed using Fast Fourier Transform on 1-s segments of data after applying multitaper. Source localization was performed using DICS (<a href=\"#bib21\">Gross et al., 2001</a>) and beamformer coefficients were computed sequentially for the frequency range from 1\u201310\u2009Hz."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-9", 
                        "title": "Coherence between lip movement signals and MEG signals", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "In this study, we used coherence as a frequency-domain measure of dependency to study how rhythmic components in lip movements in continuous speech entrain neuronal oscillations."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "First, frequency-specific brain activation time-series were computed by applying the beamformer coefficients to the MEG data filtered in the same frequency band (fourth order Butterworth filter, forward and reverse, center frequency \u00b1 3\u2009Hz). The lip speech signals were filtered in the same frequency band. Then, coherence was computed (<a href=\"#bib46\">Rosenberg et al., 1989</a>) between the lip speech signal and source-localized brain signal for each voxel and each frequency band across 1-s-long data segments overlapping by 0.5\u2009s (<a href=\"#fig2s1\">Figure 2\u2014figure supplement 1D</a>). This computation resulted in a volumetric map describing lip-entrained brain oscillations for each frequency band in each individual. This computation was performed for all experimental conditions: All congruent, All incongruent, AV congruent, AV incongruent."
                            }, 
                            {
                                "type": "paragraph", 
                                "text": "In addition, surrogate maps were created by computing coherence between brain signals and 30\u2009s-shifted lip speech signals for each of the four experimental conditions, thus destroy existing temporal dependencies between the two signals. This surrogate data serves as control data as computations use the same data after controlled manipulation that destroys the effect of interest (here the temporal shift removes temporal dependencies in coherence measure). Surrogate data therefore provide an estimate of coherence that can be expected by chance for each condition."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-10", 
                        "title": "Partial coherence", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "In audiovisual speech, auditory and visual inputs are coherent (<a href=\"#bib10\">Chandrasekaran et al., 2009</a>) (also shown in <a href=\"#fig2\">Figure 2C</a>). The main purpose of this study was to investigate whether visual signals (lip movements) entrain/modulate brain activity and where this entrainment occurs. In order to rule out functional coupling (coherence) explained by auditory signals, here we additionally computed partial coherence (<a href=\"#bib47\">Rosenberg et al., 1998</a>), i.e., the coherence partialling out sound speech signals. The analysis process was identical to the process for the coherence above. This partial coherence provides entrained brain activity explained by lip movements that cannot be accounted for by auditory speech signal."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-11", 
                        "title": "Coherence between lip movements and speech signals", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "As explained above, in natural audiovisual speech, auditory and visual information are robustly correlated (<a href=\"#bib10\">Chandrasekaran et al., 2009</a>). Here we computed the coherence between lip speech (visual) and sound speech (auditory) signals in all experimental conditions except All incongruent (All incongruent does not have matching lip-sound signals). Further, we computed coherence between non-matching lip and sound signals in all conditions except All congruent (All congruent does not have non-matching lip-sound signals)."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-12", 
                        "title": "Statistics", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "Statistical analysis was performed on the data of all 44 participants using non-parametric randomization statistics in FieldTrip (Monte Carlo randomization). Specifically, individual volumetric maps were smoothed with a 10-mm Gaussian kernel and subjected to dependent samples <i>t</i>-test. We compared each condition with corresponding surrogate data and other experimental conditions. The null distribution was estimated using 500 randomizations and multiple comparison correction was performed using FDR (False Discovery Rate) (<a href=\"#bib15\">Genovese et al., 2002</a>). Only significant results (p&lt;0.05, FDR corrected) are reported."
                            }
                        ]
                    }, 
                    {
                        "type": "section", 
                        "id": "s4-13", 
                        "title": "Regression analysis using behavioral data", 
                        "content": [
                            {
                                "type": "paragraph", 
                                "text": "To study the relationship between lip movement entrainment and behavioral performance, we performed regression analysis across subjects using comprehension accuracy from each individual as regressors on the partial coherence map. In this regression analysis, we detected brain regions positively predicted by comprehension accuracy. To maximize sensitivity of this analysis, we compared AV congruent condition to the condition that showed strongest difference in behavioral performance \u2013 AV incongruent (<a href=\"#fig1\">Figure 1B</a>). We performed the regression analysis for each condition, then <i>t</i>-values at each brain voxel from the regression analysis were transformed to standard <i>Z</i>-values to be compared between conditions. Then the <i>Z</i>-values were subtracted between the two conditions (p&lt;0.005)."
                            }
                        ]
                    }
                ]
            }
        ], 
        "references": [
            {
                "type": "journal", 
                "id": "bib1", 
                "date": "2001", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Ahissar", 
                            "index": "Ahissar, E"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Nagarajan", 
                            "index": "Nagarajan, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Ahissar", 
                            "index": "Ahissar, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Protopapas", 
                            "index": "Protopapas, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Mahncke", 
                            "index": "Mahncke, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MM Merzenich", 
                            "index": "Merzenich, MM"
                        }
                    }
                ], 
                "articleTitle": "Speech comprehension is correlated with temporal response patterns recorded from auditory cortex", 
                "journal": "Proceedings of the National Academy of Sciences of the United States of America", 
                "volume": "98", 
                "pages": {
                    "first": "13367", 
                    "last": "13372", 
                    "range": "13367\u201313372"
                }, 
                "doi": "10.1073/pnas.201400998"
            }, 
            {
                "type": "journal", 
                "id": "bib2", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "LH Arnal", 
                            "index": "Arnal, LH"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AL Giraud", 
                            "index": "Giraud, AL"
                        }
                    }
                ], 
                "articleTitle": "Cortical oscillations and sensory predictions", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "16", 
                "pages": {
                    "first": "390", 
                    "last": "398", 
                    "range": "390\u2013398"
                }, 
                "doi": "10.1016/j.tics.2012.05.003"
            }, 
            {
                "type": "journal", 
                "id": "bib3", 
                "date": "2004", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MS Beauchamp", 
                            "index": "Beauchamp, MS"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "KE Lee", 
                            "index": "Lee, KE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "BD Argall", 
                            "index": "Argall, BD"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Martin", 
                            "index": "Martin, A"
                        }
                    }
                ], 
                "articleTitle": "Integration of auditory and visual information about objects in superior temporal sulcus", 
                "journal": "Neuron", 
                "volume": "41", 
                "pages": {
                    "first": "809", 
                    "last": "823", 
                    "range": "809\u2013823"
                }, 
                "doi": "10.1016/S0896-6273(04)00070-4"
            }, 
            {
                "type": "conference-proceeding", 
                "id": "bib4", 
                "date": "2008", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Behne", 
                            "index": "Behne, D"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Y Wang", 
                            "index": "Wang, Y"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SO Belsby", 
                            "index": "Belsby, SO"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Kaasa", 
                            "index": "Kaasa, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "L Simonsen", 
                            "index": "Simonsen, L"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "K Back", 
                            "index": "Back, K"
                        }
                    }
                ], 
                "articleTitle": "Visual field advantage in the perception of audiovisual speech segments", 
                "conference": {
                    "name": [
                        "International Conference on Auditory-Visual Speech Processing. AVSP-2008"
                    ]
                }, 
                "pages": {
                    "first": "47", 
                    "last": "50", 
                    "range": "47\u201350"
                }
            }, 
            {
                "type": "journal", 
                "id": "bib5", 
                "date": "1992", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "PJ Besl", 
                            "index": "Besl, PJ"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "HD McKay", 
                            "index": "McKay, HD"
                        }
                    }
                ], 
                "articleTitle": "A method for registration of 3-D shapes", 
                "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", 
                "volume": "14", 
                "pages": {
                    "first": "239", 
                    "last": "256", 
                    "range": "239\u2013256"
                }, 
                "doi": "10.1109/34.121791"
            }, 
            {
                "type": "journal", 
                "id": "bib6", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Blank", 
                            "index": "Blank, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "K von Kriegstein", 
                            "index": "von Kriegstein, K"
                        }
                    }
                ], 
                "articleTitle": "Mechanisms of enhancing visual\u2013speech recognition by prior auditory information", 
                "journal": "NeuroImage", 
                "volume": "65", 
                "pages": {
                    "first": "109", 
                    "last": "118", 
                    "range": "109\u2013118"
                }, 
                "doi": "10.1016/j.neuroimage.2012.09.047"
            }, 
            {
                "type": "journal", 
                "id": "bib7", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "I Bornkessel-Schlesewsky", 
                            "index": "Bornkessel-Schlesewsky, I"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Schlesewsky", 
                            "index": "Schlesewsky, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SL Small", 
                            "index": "Small, SL"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JP Rauschecker", 
                            "index": "Rauschecker, JP"
                        }
                    }
                ], 
                "articleTitle": "Neurobiological roots of language in primate audition: Common computational properties", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "19", 
                "pages": {
                    "first": "142", 
                    "last": "150", 
                    "range": "142\u2013150"
                }, 
                "doi": "10.1016/j.tics.2014.12.008"
            }, 
            {
                "type": "journal", 
                "id": "bib8", 
                "date": "1997", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DH Brainard", 
                            "index": "Brainard, DH"
                        }
                    }
                ], 
                "articleTitle": "The psychophysics toolbox", 
                "journal": "Spatial Vision", 
                "volume": "10", 
                "pages": {
                    "first": "433", 
                    "last": "436", 
                    "range": "433\u2013436"
                }, 
                "doi": "10.1163/156856897X00357"
            }, 
            {
                "type": "journal", 
                "id": "bib9", 
                "date": "1996", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Campbell", 
                            "index": "Campbell, R"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B De Gelder", 
                            "index": "De Gelder, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E De Haan", 
                            "index": "De Haan, E"
                        }
                    }
                ], 
                "articleTitle": "The lateralization of lip-reading: A second look", 
                "journal": "Neuropsychologia", 
                "volume": "34", 
                "pages": {
                    "first": "1235", 
                    "last": "1240", 
                    "range": "1235\u20131240"
                }, 
                "doi": "10.1016/0028-3932(96)00046-2"
            }, 
            {
                "type": "journal", 
                "id": "bib10", 
                "date": "2009", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Chandrasekaran", 
                            "index": "Chandrasekaran, C"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Trubanova", 
                            "index": "Trubanova, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Stillittano", 
                            "index": "Stillittano, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Caplier", 
                            "index": "Caplier, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AA Ghazanfar", 
                            "index": "Ghazanfar, AA"
                        }
                    }
                ], 
                "articleTitle": "The natural statistics of audiovisual speech", 
                "journal": "PLoS Computational Biology", 
                "volume": "5", 
                "pages": "e1000436", 
                "doi": "10.1371/journal.pcbi.1000436"
            }, 
            {
                "type": "journal", 
                "id": "bib11", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MJ Crosse", 
                            "index": "Crosse, MJ"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JS Butler", 
                            "index": "Butler, JS"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "EC Lalor", 
                            "index": "Lalor, EC"
                        }
                    }
                ], 
                "articleTitle": "Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions", 
                "journal": "The Journal of Neuroscience\u00a0", 
                "volume": "35", 
                "pages": {
                    "first": "14195", 
                    "last": "14204", 
                    "range": "14195\u201314204"
                }, 
                "doi": "10.1523/JNEUROSCI.1829-15.2015"
            }, 
            {
                "type": "journal", 
                "id": "bib12", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "N Ding", 
                            "index": "Ding, N"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JZ Simon", 
                            "index": "Simon, JZ"
                        }
                    }
                ], 
                "articleTitle": "Neural coding of continuous speech in auditory cortex during monaural and dichotic listening", 
                "journal": "Journal of Neurophysiology", 
                "volume": "107", 
                "pages": {
                    "first": "78", 
                    "last": "89", 
                    "range": "78\u201389"
                }, 
                "doi": "10.1152/jn.00297.2011"
            }, 
            {
                "type": "journal", 
                "id": "bib13", 
                "date": "2014", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "N Ding", 
                            "index": "Ding, N"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JZ Simon", 
                            "index": "Simon, JZ"
                        }
                    }
                ], 
                "articleTitle": "Cortical entrainment to continuous speech: Functional roles and interpretations", 
                "journal": "Frontiers in Human Neuroscience", 
                "volume": "8", 
                "pages": "311", 
                "doi": "10.3389/fnhum.2014.00311"
            }, 
            {
                "type": "unknown", 
                "id": "bib14", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Evans", 
                            "index": "Evans, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MH Davis", 
                            "index": "Davis, MH"
                        }
                    }
                ], 
                "title": "Hierarchical organization of auditory and motor representations in speech perception: Evidence from searchlight similarity analysis", 
                "details": "Cerebral Cortex\u00a0, 25, 10.1093/cercor/bhv136"
            }, 
            {
                "type": "journal", 
                "id": "bib15", 
                "date": "2002", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CR Genovese", 
                            "index": "Genovese, CR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "NA Lazar", 
                            "index": "Lazar, NA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "T Nichols", 
                            "index": "Nichols, T"
                        }
                    }
                ], 
                "articleTitle": "Thresholding of statistical maps in functional neuroimaging using the false discovery rate", 
                "journal": "NeuroImage", 
                "volume": "15", 
                "pages": {
                    "first": "870", 
                    "last": "878", 
                    "range": "870\u2013878"
                }, 
                "doi": "10.1006/nimg.2001.1037"
            }, 
            {
                "type": "journal", 
                "id": "bib16", 
                "date": "2007", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AL Giraud", 
                            "index": "Giraud, AL"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Kleinschmidt", 
                            "index": "Kleinschmidt, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "TE Lund", 
                            "index": "Lund, TE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RS Frackowiak", 
                            "index": "Frackowiak, RS"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Laufs", 
                            "index": "Laufs, H"
                        }
                    }
                ], 
                "articleTitle": "Endogenous cortical rhythms determine cerebral specialization for speech perception and production", 
                "journal": "Neuron", 
                "volume": "56", 
                "pages": {
                    "first": "1127", 
                    "last": "1134", 
                    "range": "1127\u20131134"
                }, 
                "doi": "10.1016/j.neuron.2007.09.038"
            }, 
            {
                "type": "journal", 
                "id": "bib17", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AL Giraud", 
                            "index": "Giraud, AL"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Cortical oscillations and speech processing: Emerging computational principles and operations", 
                "journal": "Nature Neuroscience", 
                "volume": "15", 
                "pages": {
                    "first": "511", 
                    "last": "517", 
                    "range": "511\u2013517"
                }, 
                "doi": "10.1038/nn.3063"
            }, 
            {
                "type": "journal", 
                "id": "bib18", 
                "date": "2000", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "KW Grant", 
                            "index": "Grant, KW"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "PF Seitz", 
                            "index": "Seitz, PF"
                        }
                    }
                ], 
                "articleTitle": "The use of visible speech cues for improving auditory detection of spoken sentences", 
                "journal": "The Journal of the Acoustical Society of America", 
                "volume": "108", 
                "pages": {
                    "first": "1197", 
                    "last": "1208", 
                    "range": "1197\u20131208"
                }, 
                "doi": "10.1121/1.1288668"
            }, 
            {
                "type": "journal", 
                "id": "bib19", 
                "date": "2013", 
                "discriminator": "a", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Baillet", 
                            "index": "Baillet, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "GR Barnes", 
                            "index": "Barnes, GR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RN Henson", 
                            "index": "Henson, RN"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Hillebrand", 
                            "index": "Hillebrand, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "O Jensen", 
                            "index": "Jensen, O"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "K Jerbi", 
                            "index": "Jerbi, K"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "V Litvak", 
                            "index": "Litvak, V"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B Maess", 
                            "index": "Maess, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Oostenveld", 
                            "index": "Oostenveld, R"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "L Parkkonen", 
                            "index": "Parkkonen, L"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JR Taylor", 
                            "index": "Taylor, JR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "V van Wassenhove", 
                            "index": "van Wassenhove, V"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Wibral", 
                            "index": "Wibral, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J-M Schoffelen", 
                            "index": "Schoffelen, J-M"
                        }
                    }
                ], 
                "articleTitle": "Good practice for conducting and reporting MEG research", 
                "journal": "NeuroImage", 
                "volume": "65", 
                "pages": {
                    "first": "349", 
                    "last": "363", 
                    "range": "349\u2013363"
                }, 
                "doi": "10.1016/j.neuroimage.2012.10.001"
            }, 
            {
                "type": "journal", 
                "id": "bib20", 
                "date": "2013", 
                "discriminator": "b", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "N Hoogenboom", 
                            "index": "Hoogenboom, N"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Thut", 
                            "index": "Thut, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Schyns", 
                            "index": "Schyns, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Panzeri", 
                            "index": "Panzeri, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Belin", 
                            "index": "Belin, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Garrod", 
                            "index": "Garrod, S"
                        }
                    }
                ], 
                "articleTitle": "Speech rhythms and multiplexed oscillatory sensory coding in the human brain", 
                "journal": "PLoS Biology", 
                "volume": "11", 
                "pages": "e1001752", 
                "doi": "10.1371/journal.pbio.1001752"
            }, 
            {
                "type": "journal", 
                "id": "bib21", 
                "date": "2001", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Kujala", 
                            "index": "Kujala, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Hamalainen", 
                            "index": "Hamalainen, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "L Timmermann", 
                            "index": "Timmermann, L"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Schnitzler", 
                            "index": "Schnitzler, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Salmelin", 
                            "index": "Salmelin, R"
                        }
                    }
                ], 
                "articleTitle": "Dynamic imaging of coherent sources: Studying neural interactions in the human brain", 
                "journal": "Proceedings of the National Academy of Sciences of the United States of America", 
                "volume": "98", 
                "pages": {
                    "first": "694", 
                    "last": "699", 
                    "range": "694\u2013699"
                }, 
                "doi": "10.1073/pnas.98.2.694"
            }, 
            {
                "type": "journal", 
                "id": "bib22", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "U Hasson", 
                            "index": "Hasson, U"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AA Ghazanfar", 
                            "index": "Ghazanfar, AA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B Galantucci", 
                            "index": "Galantucci, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Garrod", 
                            "index": "Garrod, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Keysers", 
                            "index": "Keysers, C"
                        }
                    }
                ], 
                "articleTitle": "Brain-to-brain coupling: A mechanism for creating and sharing a social world", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "16", 
                "pages": {
                    "first": "114", 
                    "last": "121", 
                    "range": "114\u2013121"
                }, 
                "doi": "10.1016/j.tics.2011.12.007"
            }, 
            {
                "type": "journal", 
                "id": "bib23", 
                "date": "2008", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Hocking", 
                            "index": "Hocking, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CJ Price", 
                            "index": "Price, CJ"
                        }
                    }
                ], 
                "articleTitle": "The role of the posterior superior temporal sulcus in audiovisual processing", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "18", 
                "pages": {
                    "first": "2439", 
                    "last": "2449", 
                    "range": "2439\u20132449"
                }, 
                "doi": "10.1093/cercor/bhn007"
            }, 
            {
                "type": "journal", 
                "id": "bib24", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SJ Kayser", 
                            "index": "Kayser, SJ"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RA Ince", 
                            "index": "Ince, RA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Kayser", 
                            "index": "Kayser, C"
                        }
                    }
                ], 
                "articleTitle": "Irregular speech rate dissociates auditory cortical entrainment, evoked responses, and frontal alpha", 
                "journal": "The Journal of Neuroscience\u00a0", 
                "volume": "35", 
                "pages": {
                    "first": "14691", 
                    "last": "14701", 
                    "range": "14691\u201314701"
                }, 
                "doi": "10.1523/JNEUROSCI.2243-15.2015"
            }, 
            {
                "type": "journal", 
                "id": "bib25", 
                "date": "2003", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Kim", 
                            "index": "Kim, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Davis", 
                            "index": "Davis, C"
                        }
                    }
                ], 
                "articleTitle": "Hearing foreign voices: Does knowing what is said affect visual-masked-speech detection?", 
                "journal": "Perception", 
                "volume": "32", 
                "pages": {
                    "first": "111", 
                    "last": "120", 
                    "range": "111\u2013120"
                }, 
                "doi": "10.1068/p3466"
            }, 
            {
                "type": "journal", 
                "id": "bib26", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Lakatos", 
                            "index": "Lakatos, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Musacchia", 
                            "index": "Musacchia, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MN O'Connel", 
                            "index": "O'Connel, MN"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AY Falchier", 
                            "index": "Falchier, AY"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DC Javitt", 
                            "index": "Javitt, DC"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }
                ], 
                "articleTitle": "The spectrotemporal filter mechanism of auditory selective attention", 
                "journal": "Neuron", 
                "volume": "77", 
                "pages": {
                    "first": "750", 
                    "last": "761", 
                    "range": "750\u2013761"
                }, 
                "doi": "10.1016/j.neuron.2012.11.034"
            }, 
            {
                "type": "journal", 
                "id": "bib27", 
                "date": "1932", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Likert", 
                            "index": "Likert, R"
                        }
                    }
                ], 
                "articleTitle": "A technique for the measurement of attitudes", 
                "journal": "Archives of Psychology", 
                "volume": "22", 
                "pages": {
                    "first": "1", 
                    "last": "55", 
                    "range": "1\u201355"
                }
            }, 
            {
                "type": "journal", 
                "id": "bib28", 
                "date": "2010", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Luo", 
                            "index": "Luo, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Z Liu", 
                            "index": "Liu, Z"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation", 
                "journal": "PLoS Biology", 
                "volume": "8", 
                "pages": "e1000445", 
                "doi": "10.1371/journal.pbio.1000445"
            }, 
            {
                "type": "journal", 
                "id": "bib29", 
                "date": "2007", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Luo", 
                            "index": "Luo, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex", 
                "journal": "Neuron", 
                "volume": "54", 
                "pages": {
                    "first": "1001", 
                    "last": "1010", 
                    "range": "1001\u20131010"
                }, 
                "doi": "10.1016/j.neuron.2007.06.004"
            }, 
            {
                "type": "journal", 
                "id": "bib30", 
                "date": "2007", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "IG Meister", 
                            "index": "Meister, IG"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SM Wilson", 
                            "index": "Wilson, SM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Deblieck", 
                            "index": "Deblieck, C"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AD Wu", 
                            "index": "Wu, AD"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Iacoboni", 
                            "index": "Iacoboni, M"
                        }
                    }
                ], 
                "articleTitle": "The essential role of premotor cortex in speech perception", 
                "journal": "Current Biology : CB", 
                "volume": "17", 
                "pages": {
                    "first": "1692", 
                    "last": "1696", 
                    "range": "1692\u20131696"
                }, 
                "doi": "10.1016/j.cub.2007.08.064"
            }, 
            {
                "type": "journal", 
                "id": "bib31", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B Morillon", 
                            "index": "Morillon, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "TA Hackett", 
                            "index": "Hackett, TA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Y Kajikawa", 
                            "index": "Kajikawa, Y"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }
                ], 
                "articleTitle": "Predictive motor control of sensory dynamics in auditory active sensing", 
                "journal": "Current Opinion in Neurobiology", 
                "volume": "31", 
                "pages": {
                    "first": "230", 
                    "last": "238", 
                    "range": "230\u2013238"
                }, 
                "doi": "10.1016/j.conb.2014.12.005"
            }, 
            {
                "type": "unknown", 
                "id": "bib32", 
                "date": "2014", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B Morillon", 
                            "index": "Morillon, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "V Wyart", 
                            "index": "Wyart, V"
                        }
                    }
                ], 
                "title": "Motor contributions to the temporal precision of auditory attention", 
                "details": "Nature Communications, 5, 10.1038/ncomms6255"
            }, 
            {
                "type": "journal", 
                "id": "bib33", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R M\u00f6tt\u00f6nen", 
                            "index": "M\u00f6tt\u00f6nen, R"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Dutton", 
                            "index": "Dutton, R"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "KE Watkins", 
                            "index": "Watkins, KE"
                        }
                    }
                ], 
                "articleTitle": "Auditory-motor processing of speech sounds", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "23", 
                "pages": {
                    "first": "1190", 
                    "last": "1197", 
                    "range": "1190\u20131197"
                }, 
                "doi": "10.1093/cercor/bhs110"
            }, 
            {
                "type": "journal", 
                "id": "bib34", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "BS Ng", 
                            "index": "Ng, BS"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "NK Logothetis", 
                            "index": "Logothetis, NK"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Kayser", 
                            "index": "Kayser, C"
                        }
                    }
                ], 
                "articleTitle": "EEG phase patterns reflect the selectivity of neural firing", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "23", 
                "pages": {
                    "first": "389", 
                    "last": "398", 
                    "range": "389\u2013398"
                }, 
                "doi": "10.1093/cercor/bhs031"
            }, 
            {
                "type": "journal", 
                "id": "bib35", 
                "date": "2006", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "ME Nicholls", 
                            "index": "Nicholls, ME"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DA Searle", 
                            "index": "Searle, DA"
                        }
                    }
                ], 
                "articleTitle": "Asymmetries for the visual expression and perception of speech", 
                "journal": "Brain and Language", 
                "volume": "97", 
                "pages": {
                    "first": "322", 
                    "last": "331", 
                    "range": "322\u2013331"
                }, 
                "doi": "10.1016/j.bandl.2005.11.007"
            }, 
            {
                "type": "journal", 
                "id": "bib36", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "T Noesselt", 
                            "index": "Noesselt, T"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Bergmann", 
                            "index": "Bergmann, D"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "HJ Heinze", 
                            "index": "Heinze, HJ"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "T M\u00fcnte", 
                            "index": "M\u00fcnte, T"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Spence", 
                            "index": "Spence, C"
                        }
                    }
                ], 
                "articleTitle": "Coding of multisensory temporal patterns in human superior temporal sulcus", 
                "journal": "Frontiers in Integrative Neuroscience", 
                "volume": "6", 
                "pages": "64", 
                "doi": "10.3389/fnint.2012.00064"
            }, 
            {
                "type": "journal", 
                "id": "bib37", 
                "date": "2003", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Nolte", 
                            "index": "Nolte, G"
                        }
                    }
                ], 
                "articleTitle": "The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors", 
                "journal": "Physics in Medicine and Biology", 
                "volume": "48", 
                "pages": {
                    "first": "3637", 
                    "last": "3652", 
                    "range": "3637\u20133652"
                }, 
                "doi": "10.1088/0031-9155/48/22/002"
            }, 
            {
                "type": "journal", 
                "id": "bib38", 
                "date": "1971", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RC Oldfield", 
                            "index": "Oldfield, RC"
                        }
                    }
                ], 
                "articleTitle": "The assessment and analysis of handedness: the Edinburgh inventory", 
                "journal": "Neuropsychologia", 
                "volume": "9", 
                "pages": {
                    "first": "97", 
                    "last": "113", 
                    "range": "97\u2013113"
                }, 
                "doi": "10.1016/0028-3932(71)90067-4"
            }, 
            {
                "type": "journal", 
                "id": "bib39", 
                "date": "2011", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "R Oostenveld", 
                            "index": "Oostenveld, R"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Fries", 
                            "index": "Fries, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Maris", 
                            "index": "Maris, E"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JM Schoffelen", 
                            "index": "Schoffelen, JM"
                        }
                    }
                ], 
                "articleTitle": "Fieldtrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data", 
                "journal": "Computational Intelligence and Neuroscience", 
                "volume": "2011", 
                "pages": "156869", 
                "doi": "10.1155/2011/156869"
            }, 
            {
                "type": "journal", 
                "id": "bib40", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Panzeri", 
                            "index": "Panzeri, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JH Macke", 
                            "index": "Macke, JH"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Kayser", 
                            "index": "Kayser, C"
                        }
                    }
                ], 
                "articleTitle": "Neural population coding: Combining insights from microscopic and mass signals", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "19", 
                "pages": {
                    "first": "162", 
                    "last": "172", 
                    "range": "162\u2013172"
                }, 
                "doi": "10.1016/j.tics.2015.01.002"
            }, 
            {
                "type": "journal", 
                "id": "bib41", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Park", 
                            "index": "Park, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RA Ince", 
                            "index": "Ince, RA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "PG Schyns", 
                            "index": "Schyns, PG"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Thut", 
                            "index": "Thut, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }
                ], 
                "articleTitle": "Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners", 
                "journal": "Current Biology\u00a0", 
                "volume": "25", 
                "pages": {
                    "first": "1649", 
                    "last": "1653", 
                    "range": "1649\u20131653"
                }, 
                "doi": "10.1016/j.cub.2015.04.049"
            }, 
            {
                "type": "journal", 
                "id": "bib42", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JE Peelle", 
                            "index": "Peelle, JE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MH Davis", 
                            "index": "Davis, MH"
                        }
                    }
                ], 
                "articleTitle": "Phase-locked responses to speech in human auditory cortex are enhanced during comprehension", 
                "journal": "Cerebral Cortex", 
                "volume": "23", 
                "pages": {
                    "first": "1378", 
                    "last": "1387", 
                    "range": "1378\u20131387"
                }, 
                "doi": "10.1093/cercor/bhs118"
            }, 
            {
                "type": "journal", 
                "id": "bib43", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JE Peelle", 
                            "index": "Peelle, JE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MS Sommers", 
                            "index": "Sommers, MS"
                        }
                    }
                ], 
                "articleTitle": "Prediction and constraint in audiovisual speech perception", 
                "journal": "Cortex", 
                "volume": "68", 
                "pages": {
                    "first": "169", 
                    "last": "181", 
                    "range": "169\u2013181"
                }, 
                "doi": "10.1016/j.cortex.2015.03.006"
            }, 
            {
                "type": "journal", 
                "id": "bib44", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MJ Pickering", 
                            "index": "Pickering, MJ"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Garrod", 
                            "index": "Garrod, S"
                        }
                    }
                ], 
                "articleTitle": "An integrated theory of language production and comprehension", 
                "journal": "The Behavioral and Brain Sciences", 
                "volume": "36", 
                "pages": {
                    "first": "329", 
                    "last": "347", 
                    "range": "329\u2013347"
                }, 
                "doi": "10.1017/S0140525X12001495"
            }, 
            {
                "type": "journal", 
                "id": "bib45", 
                "date": "2009", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JP Rauschecker", 
                            "index": "Rauschecker, JP"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SK Scott", 
                            "index": "Scott, SK"
                        }
                    }
                ], 
                "articleTitle": "Maps and streams in the auditory cortex: Nonhuman primates illuminate human speech processing", 
                "journal": "Nature Neuroscience", 
                "volume": "12", 
                "pages": {
                    "first": "718", 
                    "last": "724", 
                    "range": "718\u2013724"
                }, 
                "doi": "10.1038/nn.2331"
            }, 
            {
                "type": "journal", 
                "id": "bib46", 
                "date": "1989", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JR Rosenberg", 
                            "index": "Rosenberg, JR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AM Amjad", 
                            "index": "Amjad, AM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Breeze", 
                            "index": "Breeze, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DR Brillinger", 
                            "index": "Brillinger, DR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DM Halliday", 
                            "index": "Halliday, DM"
                        }
                    }
                ], 
                "articleTitle": "The fourier approach to the identification of functional coupling between neuronal spike trains", 
                "journal": "Progress in Biophysics and Molecular Biology", 
                "volume": "53", 
                "pages": {
                    "first": "1", 
                    "last": "31", 
                    "range": "1\u201331"
                }, 
                "doi": "10.1016/0079-6107(89)90004-7"
            }, 
            {
                "type": "journal", 
                "id": "bib47", 
                "date": "1998", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JR Rosenberg", 
                            "index": "Rosenberg, JR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DM Halliday", 
                            "index": "Halliday, DM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Breeze", 
                            "index": "Breeze, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "BA Conway", 
                            "index": "Conway, BA"
                        }
                    }
                ], 
                "articleTitle": "Identification of patterns of neuronal connectivity--partial spectra, partial coherence, and neuronal interactions", 
                "journal": "Journal of Neuroscience Methods", 
                "volume": "83", 
                "pages": {
                    "first": "57", 
                    "last": "72", 
                    "range": "57\u201372"
                }, 
                "doi": "10.1016/S0165-0270(98)00061-2"
            }, 
            {
                "type": "journal", 
                "id": "bib48", 
                "date": "2008", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Lakatos", 
                            "index": "Lakatos, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Y Kajikawa", 
                            "index": "Kajikawa, Y"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Partan", 
                            "index": "Partan, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Puce", 
                            "index": "Puce, A"
                        }
                    }
                ], 
                "articleTitle": "Neuronal oscillations and visual amplification of speech", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "12", 
                "pages": {
                    "first": "106", 
                    "last": "113", 
                    "range": "106\u2013113"
                }, 
                "doi": "10.1016/j.tics.2008.01.002"
            }, 
            {
                "type": "journal", 
                "id": "bib49", 
                "date": "2011", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "PG Schyns", 
                            "index": "Schyns, PG"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Thut", 
                            "index": "Thut, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }
                ], 
                "articleTitle": "Cracking the code of oscillatory activity", 
                "journal": "PLoS Biology", 
                "volume": "9", 
                "pages": "e1001064", 
                "doi": "10.1371/journal.pbio.1001064"
            }, 
            {
                "type": "journal", 
                "id": "bib50", 
                "date": "1998", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "PM Smeele", 
                            "index": "Smeele, PM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "DW Massaro", 
                            "index": "Massaro, DW"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MM Cohen", 
                            "index": "Cohen, MM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AC Sittig", 
                            "index": "Sittig, AC"
                        }
                    }
                ], 
                "articleTitle": "Laterality in visual speech perception", 
                "journal": "Journal of Experimental Psychology. Human Perception and Performance", 
                "volume": "24", 
                "pages": {
                    "first": "1232", 
                    "last": "1242", 
                    "range": "1232\u20131242"
                }, 
                "doi": "10.1037/0096-1523.24.4.1232"
            }, 
            {
                "type": "journal", 
                "id": "bib51", 
                "date": "2002", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "ZM Smith", 
                            "index": "Smith, ZM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "B Delgutte", 
                            "index": "Delgutte, B"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AJ Oxenham", 
                            "index": "Oxenham, AJ"
                        }
                    }
                ], 
                "articleTitle": "Chimaeric sounds reveal dichotomies in auditory perception", 
                "journal": "Nature", 
                "volume": "416", 
                "pages": {
                    "first": "87", 
                    "last": "90", 
                    "range": "87\u201390"
                }, 
                "doi": "10.1038/416087a"
            }, 
            {
                "type": "journal", 
                "id": "bib52", 
                "date": "1954", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "WH Sumby", 
                            "index": "Sumby, WH"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "I Pollack", 
                            "index": "Pollack, I"
                        }
                    }
                ], 
                "articleTitle": "Visual contribution to speech intelligibility in noise", 
                "journal": "The Journal of the Acoustical Society of America", 
                "volume": "26", 
                "pages": "212", 
                "doi": "10.1121/1.1907309"
            }, 
            {
                "type": "journal", 
                "id": "bib53", 
                "date": "2008", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Swerts", 
                            "index": "Swerts, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Krahmer", 
                            "index": "Krahmer, E"
                        }
                    }
                ], 
                "articleTitle": "Facial expression and prosodic prominence: Effects of modality and facial area", 
                "journal": "Journal of Phonetics", 
                "volume": "36", 
                "pages": {
                    "first": "219", 
                    "last": "238", 
                    "range": "219\u2013238"
                }, 
                "doi": "10.1016/j.wocn.2007.05.001"
            }, 
            {
                "type": "journal", 
                "id": "bib54", 
                "date": "2004", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "LA Thompson", 
                            "index": "Thompson, LA"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Malmberg", 
                            "index": "Malmberg, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "NK Goodell", 
                            "index": "Goodell, NK"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "RL Boring", 
                            "index": "Boring, RL"
                        }
                    }
                ], 
                "articleTitle": "The Distribution of Attention Across a Talker's Face", 
                "journal": "Discourse Processes", 
                "volume": "38", 
                "pages": {
                    "first": "145", 
                    "last": "168", 
                    "range": "145\u2013168"
                }, 
                "doi": "10.1207/s15326950dp3801_6"
            }, 
            {
                "type": "journal", 
                "id": "bib55", 
                "date": "2005", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "V van Wassenhove", 
                            "index": "van Wassenhove, V"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "KW Grant", 
                            "index": "Grant, KW"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Visual speech speeds up the neural processing of auditory speech", 
                "journal": "Proceedings of the National Academy of Sciences of the United States of America", 
                "volume": "102", 
                "pages": {
                    "first": "1181", 
                    "last": "1186", 
                    "range": "1181\u20131186"
                }, 
                "doi": "10.1073/pnas.0408949102"
            }, 
            {
                "type": "journal", 
                "id": "bib56", 
                "date": "2003", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "KE Watkins", 
                            "index": "Watkins, KE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AP Strafella", 
                            "index": "Strafella, AP"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "T Paus", 
                            "index": "Paus, T"
                        }
                    }
                ], 
                "articleTitle": "Seeing and hearing speech excites the motor system involved in speech production", 
                "journal": "Neuropsychologia", 
                "volume": "41", 
                "pages": {
                    "first": "989", 
                    "last": "994", 
                    "range": "989\u2013994"
                }, 
                "doi": "10.1016/S0028-3932(02)00316-0"
            }, 
            {
                "type": "journal", 
                "id": "bib57", 
                "date": "2010", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Werner", 
                            "index": "Werner, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "U Noppeney", 
                            "index": "Noppeney, U"
                        }
                    }
                ], 
                "articleTitle": "Superadditive responses in superior temporal sulcus predict audiovisual benefits in object categorization", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "20", 
                "pages": {
                    "first": "1829", 
                    "last": "1842", 
                    "range": "1829\u20131842"
                }, 
                "doi": "10.1093/cercor/bhp248"
            }, 
            {
                "type": "journal", 
                "id": "bib58", 
                "date": "1975", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "UN Wiesmann", 
                            "index": "Wiesmann, UN"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S DiDonato", 
                            "index": "DiDonato, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "NN Herschkowitz", 
                            "index": "Herschkowitz, NN"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J.L Schwartz", 
                            "index": "Schwartz, J.L"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Savariaux", 
                            "index": "Savariaux, C"
                        }
                    }
                ], 
                "articleTitle": "Effect of chloroquine on cultured fibroblasts: Release of lysosomal hydrolases and inhibition of their uptake", 
                "journal": "Biochemical and Biophysical Research Communications", 
                "volume": "66", 
                "pages": "e1003743", 
                "doi": "10.1371/journal.pcbi.1003743"
            }, 
            {
                "type": "journal", 
                "id": "bib59", 
                "date": "2008", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SM Wilson", 
                            "index": "Wilson, SM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "I Molnar-Szakacs", 
                            "index": "Molnar-Szakacs, I"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Iacoboni", 
                            "index": "Iacoboni, M"
                        }
                    }
                ], 
                "articleTitle": "Beyond superior temporal cortex: Intersubject correlations in narrative speech comprehension", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "18", 
                "pages": {
                    "first": "230", 
                    "last": "242", 
                    "range": "230\u2013242"
                }, 
                "doi": "10.1093/cercor/bhm049"
            }, 
            {
                "type": "journal", 
                "id": "bib60", 
                "date": "2004", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "SM Wilson", 
                            "index": "Wilson, SM"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AP Saygin", 
                            "index": "Saygin, AP"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "MI Sereno", 
                            "index": "Sereno, MI"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Iacoboni", 
                            "index": "Iacoboni, M"
                        }
                    }
                ], 
                "articleTitle": "Listening to speech activates motor areas involved in speech production", 
                "journal": "Nature Neuroscience", 
                "volume": "7", 
                "pages": {
                    "first": "701", 
                    "last": "702", 
                    "range": "701\u2013702"
                }, 
                "doi": "10.1038/nn1263"
            }, 
            {
                "type": "journal", 
                "id": "bib61", 
                "date": "2015", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Ylinen", 
                            "index": "Ylinen, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Nora", 
                            "index": "Nora, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "A Leminen", 
                            "index": "Leminen, A"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "T Hakala", 
                            "index": "Hakala, T"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "M Huotilainen", 
                            "index": "Huotilainen, M"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Y Shtyrov", 
                            "index": "Shtyrov, Y"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "JP M\u00e4kel\u00e4", 
                            "index": "M\u00e4kel\u00e4, JP"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Service", 
                            "index": "Service, E"
                        }
                    }
                ], 
                "articleTitle": "Two distinct auditory-motor circuits for monitoring speech production as revealed by content-specific suppression of auditory cortex", 
                "journal": "Cerebral Cortex\u00a0", 
                "volume": "25", 
                "pages": {
                    "first": "1576", 
                    "last": "1586", 
                    "range": "1576\u20131586"
                }, 
                "doi": "10.1093/cercor/bht351"
            }, 
            {
                "type": "journal", 
                "id": "bib62", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Zion Golumbic", 
                            "index": "Zion Golumbic, E"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "GB Cogan", 
                            "index": "Cogan, GB"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Visual input enhances selective speech envelope tracking in auditory cortex at a \"cocktail party\"", 
                "journal": "The Journal of Neuroscience\u00a0", 
                "volume": "33", 
                "pages": {
                    "first": "1417", 
                    "last": "1426", 
                    "range": "1417\u20131426"
                }, 
                "doi": "10.1523/JNEUROSCI.3675-12.2013"
            }, 
            {
                "type": "journal", 
                "id": "bib63", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Zion-Golumbic", 
                            "index": "Zion-Golumbic, E"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }
                ], 
                "articleTitle": "Attention modulates 'speech-tracking' at a cocktail party", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "16", 
                "pages": {
                    "first": "363", 
                    "last": "364", 
                    "range": "363\u2013364"
                }, 
                "doi": "10.1016/j.tics.2012.05.004"
            }
        ], 
        "acknowledgements": [
            {
                "type": "paragraph", 
                "text": "JG is supported by the Wellcome Trust (098433). GT is supported by the Wellcome Trust (098434). CK is supported by the UK Biotechnology and Biological Sciences Research Council (BBSRC; grant No BB/L027534/1) and the European Research Council (ERC-2014-CoG; grant No 646657). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."
            }
        ], 
        "decisionLetter": {
            "doi": "10.7554/eLife.14521.017", 
            "description": [
                {
                    "type": "paragraph", 
                    "text": "In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included."
                }
            ], 
            "content": [
                {
                    "type": "paragraph", 
                    "text": "Thank you for submitting your article \"Lip movements during speech entrain observers' low-frequency brain oscillations\" for consideration by <i>eLife</i>. Your article has been favorably evaluated by Jody Culham as the Senior editor and three reviewers, one of whom is a member of our Board of Reviewing Editors."
                }, 
                {
                    "type": "paragraph", 
                    "text": "The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission."
                }, 
                {
                    "type": "paragraph", 
                    "text": "Summary of the work:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "This study examines the role of visual input during speech comprehension. The authors use MEG to show that cortical activity is entrained to lip movements, and that the synchronized activity is modulated by the congruence between visual and auditory inputs and possibly by attention. In addition to showing that entrainment in auditory and speech areas depends on the congruence of the auditory and visual speech signals, they found that activity in visual cortex and left motor cortex is synchronized to lip movements independently of auditory speech signals, and that the motor cortex coherence predicts comprehension accuracy. Although neural phase tracking (measured via coherence to external signals) in the low frequency domain during speech perception is a well-studied phenomenon, and previous work has demonstrated neural entrainment to visual stimuli, this study represents an important step forward by extending research in this field to the more ecologically valid condition of lip reading."
                }, 
                {
                    "type": "paragraph", 
                    "text": "Essential revision requirements:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "While the reviewers found the results compelling and concluded that they provide valuable insights into the way the brain responds to audiovisual speech, they identified the following concerns."
                }, 
                {
                    "type": "paragraph", 
                    "text": "1) The reviewers felt that the paper would benefit from a stronger theoretical underpinning. At present, it is structured as an investigation into brain oscillations, which makes it hard to keep track of what are the main questions and findings. Some reorganization of the paper would help here."
                }, 
                {
                    "type": "paragraph", 
                    "text": "2) The claim made at several points in the manuscript that lip movements entrain activity in auditory cortical areas does not appear to be justified. Although increased coherence was found there in the AV congruent vs. All incongruent condition, indicative of an interaction between the two modalities, lip movements in the absence of auditory speech (<a href=\"#fig2s2\">Figure 2\u2014figure supplement 2B</a>) or with the contribution of speech signals removed (<a href=\"#fig4\">Figure 4B</a>) do not entrain auditory cortical activity. Thus, the effects on auditory cortex are only present during speech. The use of a partial correlation analysis to demonstrate that neural entrainment to lip movements is not an artifact caused by neural entrainment to the sound is very important and should be used throughout, not just toward the end of the Results."
                }, 
                {
                    "type": "paragraph", 
                    "text": "3) <a href=\"#fig2\">Figure 2D, E</a> shows results at 1 Hz while <a href=\"#fig3\">Figure 3</a> shows the contrast at 4 Hz. It is stated that <a href=\"#fig2\">Figure 2D, E</a> focused on 1 Hz since significant entrainment was seen \"especially at 1 Hz\". Does that mean stronger coherence or broader activation? It would be helpful to show the coherence spectrum of representative areas or to show the source distribution at several frequencies. Additionally, for the source-space plots, showing the differences between the actual coherence values could be more informative."
                }, 
                {
                    "type": "paragraph", 
                    "text": "4) Visual attention was not explicitly manipulated in this study. It is assumed that visual attention is enhanced when a competing sound is presented. Although this is a reasonable interpretation, it is not appropriate to report the auditory masking effect quite so definitively as a visual attention effect. Furthermore, the All incongruent condition should be the most attention demanding condition since there are two independent distractors, yet no regions show this in the contrast (<a href=\"#fig3\">Figure 3D</a>). Please comment on this. Is this true for the all incongruent vs. all congruent contrast?"
                }, 
                {
                    "type": "paragraph", 
                    "text": "5) Comparison with the behavioral data is based on the contrast between the AV congruent condition and the AV incongruent condition. Why did you not examine the contrast between AV congruent and the All incongruent condition? Was regression done on the contrast or was the contrast done on the regression results? It is not clear either why further analysis then relies on the AV congruent condition rather than the contrast. Were other conditions also analyzed?"
                }, 
                {
                    "type": "paragraph", 
                    "text": "6) The analysis is done purely in the spectral domain. It would also be of interest to know, e.g., the latency of the visual tracking effect and whether congruent visual information shortens the latency of auditory entrainment. Such analysis would help to link the current study with the previous literature on AV integration."
                }, 
                {
                    "type": "paragraph", 
                    "text": "7) Although the relationship shown in Figure 5 between coherence in the left motor cortex and speech comprehension is very interesting, this does not show that the \"motor cortex component contributes to speech comprehension\" (Discussion), which would require demonstrating that disruption of activity in motor cortex affects speech comprehension."
                }, 
                {
                    "type": "paragraph", 
                    "text": "8) Does the motor cortex activity include the lip regions? Please provide appropriate citations to justify this."
                }
            ]
        }, 
        "authorResponse": {
            "doi": "10.7554/eLife.14521.018", 
            "content": [
                {
                    "type": "paragraph", 
                    "text": "<i>1) The reviewers felt that the paper would benefit from a stronger theoretical underpinning. At present, it is structured as an investigation into brain oscillations, which makes it hard to keep track of what are the main questions and findings. Some reorganization of the paper would help here.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We are grateful to the reviewers for the overall positive feedback to our manuscript. We have revised the logical foundation, organization and presented analyses within the revised manuscript according to the reviewer\u2019s suggestions. Importantly, we now provide a stronger theoretical underpinning by opening the Introduction with an introduction to the multisensory phenomena related to lip reading. Then, we lead over to the potential relevance of brain oscillations in this context and outline the main four hypotheses that we test in this study. These four hypotheses provide a clear structure for the presentation of the analyses and the discussion."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>2) The claim made at several points in the manuscript that lip movements entrain activity in auditory cortical areas does not appear to be justified. Although increased coherence was found there in the AV congruent vs. All incongruent condition, indicative of an interaction between the two modalities, lip movements in the absence of auditory speech (<a href=\"#fig2s2\">Figure 2\u2014figure supplement 2B</a>) or with the contribution of speech signals removed (<a href=\"#fig4\">Figure 4B</a>) do not entrain auditory cortical activity. Thus, the effects on auditory cortex are only present during speech. The use of a partial correlation analysis to demonstrate that neural entrainment to lip movements is not an artifact caused by neural entrainment to the sound is very important and should be used throughout, not just toward the end of the Results.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We agree with the reviewers that we currently have no direct evidence that lip movements entrain auditory brain areas. Although coherence between lip movements and right auditory cortex increases significantly for AV congruent compared to All congruent condition, similar effects are absent in the partial coherence analysis. In response to the reviewers comment we have removed this claim from the revised manuscript. Instead we have added a short speculative comment about the coherence increase (AV cong &gt; All cong) to the Discussion section. The relevant part reads:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "\u201cWhile visual areas show both coherence and partial coherence to lip movements, temporal areas only emerge from coherence analysis (<a href=\"#fig2\">Figure 2D</a> and <a href=\"#fig3s1\">Figure 3\u2014figure supplement 1</a>). [\u2026] However, the informativeness of lip movements still modulates significantly the coherence between lip movements and right temporal brain areas (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1B, C, D</a>) and indicates an indirect effect of visual attention.\u201d"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We have also addressed the reviewers\u2019 comments that partial coherence should be used throughout. Specifically, we have moved <a href=\"#fig3\">Figure 3</a> (coherence results) to Supplemental Materials (<a href=\"#fig3s1\">Figure 3\u2014figure supplement 1</a>) and directly focus on the partial coherence results in the main analysis. Still, we believe that traditional coherence maps convey essential information and this information is still available as Supplemental Materials."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>3) <a href=\"#fig2\">Figure 2D, E</a> shows results at 1 Hz while <a href=\"#fig3\">Figure 3</a> shows the contrast at 4 Hz. It is stated that <a href=\"#fig2\">Figure 2D, E</a> focused on 1 Hz since significant entrainment was seen \"especially at 1 Hz\". Does that mean stronger coherence or broader activation? It would be helpful to show the coherence spectrum of representative areas or to show the source distribution at several frequencies. Additionally, for the source-space plots, showing the differences between the actual coherence values could be more informative.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We now realise that the statement \u2018especially at 1 Hz\u2019 is misleading. This frequency was selected because the lip movement signal shows the strongest power at this frequency (<a href=\"#fig2\">Figure 2B</a>). The power spectrum for sound speech signals (for all the auditory stimuli we used) also shows the peak at 1 Hz as shown in <a href=\"#fig5\">Author response image 1</a>."
                }, 
                {
                    "type": "image", 
                    "doi": "10.7554/eLife.14521.012", 
                    "id": "fig5", 
                    "label": "Author response image 1.", 
                    "title": "Author response image 1.", 
                    "alt": "", 
                    "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-resp-fig1-v2.jpg", 
                    "filename": "elife-14521-resp-fig1-v2.jpg"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We have now rephrased the corresponding part in the revised manuscript:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "\u201cWe first compared natural audiovisual speech condition (All congruent) and surrogate data for the frequency that showed strongest power in the lip signal (1 Hz). This revealed a significant entrainment effect in visual, auditory, and language areas bilaterally (<i>P</i> &lt; 0.05, false discovery rate (FDR) corrected; <a href=\"#fig2\">Figure 2D</a>).\u201d"
                }, 
                {
                    "type": "paragraph", 
                    "text": "In addition, as the reviewers suggested, we have added source distribution at several frequencies corresponding to <a href=\"#fig2\">Figure 2D and E</a> as supplemental figures (<a href=\"#fig2s3\">Figure 2\u2014figure supplement 3</a>)."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<a href=\"#fig6\">Author response image 2</a> shows maps with actual coherence values for <a href=\"#fig2\">Figure 2D, E, F</a>."
                }, 
                {
                    "type": "image", 
                    "doi": "10.7554/eLife.14521.013", 
                    "id": "fig6", 
                    "label": "Author response image 2.", 
                    "title": "Author response image 2.", 
                    "alt": "", 
                    "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-resp-fig2-v2.jpg", 
                    "filename": "elife-14521-resp-fig2-v2.jpg"
                }, 
                {
                    "type": "paragraph", 
                    "text": "In <a href=\"#fig6\">Author response image 2</a> we also show maps of actual coherence value for each condition corresponding to <a href=\"#fig3s1\">Figure 3\u2014figure supplement 1A and B</a>. However, we think the graphs in the original manuscript (<i>t</i>-statistics) are more informative than the plots in <a href=\"#fig7\">Author response image 3</a>."
                }, 
                {
                    "type": "image", 
                    "doi": "10.7554/eLife.14521.014", 
                    "id": "fig7", 
                    "label": "Author response image 3.", 
                    "title": "Author response image 3.", 
                    "alt": "", 
                    "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-resp-fig3-v2.jpg", 
                    "filename": "elife-14521-resp-fig3-v2.jpg"
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>4) Visual attention was not explicitly manipulated in this study. It is assumed that visual attention is enhanced when a competing sound is presented. Although this is a reasonable interpretation, it is not appropriate to report the auditory masking effect quite so definitively as a visual attention effect. Furthermore, the All incongruent condition should be the most attention demanding condition since there are two independent distractors, yet no regions show this in the contrast (<a href=\"#fig3\">Figure 3D</a>). Please comment on this. Is this true for the all incongruent vs. all congruent contrast?</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We fully agree that we have not manipulated visual attention explicitly but only indirectly through the use of a competing sound. We have now clarified this point in the Discussion section of the revised manuscript. The relevant part reads:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "\u201cWe studied the effect of attention in an ecologically valid scenario with congruent audiovisual speech when a distracting auditory stimulus was present (AV congruent condition) or absent (All congruent condition). [\u2026] This condition was compared to the All congruent condition where attention to the visual stimulus is not required for speech comprehension.\u201d"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We believe that the All incongruent condition is not the most attention demanding condition because participants were instructed to always attend to auditory stimuli (see <a href=\"#fig1\">Figure 1A</a> where red letters represent attended stimulus). So there is no benefit for attending to the visual input in the All incongruent condition. This is different for the AV congruent condition where the congruent visual input facilitates comprehension of the attended auditory stream in the presence of another distracting auditory stream. Amongst all conditions the visual input is therefore likely most relevant and informative in the AV congruent condition. We have therefore used this condition for analysis of the \u2018visual attention\u2019 effect."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>5) Comparison with the behavioral data is based on the contrast between the AV congruent condition and the AV incongruent condition. Why did you not examine the contrast between AV congruent and the All incongruent condition? Was regression done on the contrast or was the contrast done on the regression results? It is not clear either why further analysis then relies on the AV congruent condition rather than the contrast. Were other conditions also analyzed?</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "The analysis of the behavioural relevance of lip entrainment results was based on the rationale that this effect should be strongest when we compare our main condition (AV congruent) to the condition with the largest difference in behavioural performance, which is the AV incongruent condition (see <a href=\"#fig1\">Figure 1B</a>). We performed the regression analysis for each condition and then t-values at each brain voxel from the regression analysis were transformed to standard z-values to be compared between conditions. Then the z-values were subtracted between the two conditions. We have now described the procedure in more detail in the Methods section of the revised manuscript. The relevant section reads:"
                }, 
                {
                    "type": "paragraph", 
                    "text": "\u201cTo study the relationship between lip movement entrainment and behavioral performance, we performed regression analysis across subjects using comprehension accuracy from each individual as regressors on the partial coherence map. [\u2026] Then the <i>Z</i>-values were subtracted between the two conditions (<i>P</i> &lt; 0.005).\u201d"
                }, 
                {
                    "type": "paragraph", 
                    "text": "The contrast between AV congruent and AV incongruent condition already establishes a correlation between lip-entrainment and behavioural performance. Yet, to obtain further insights into this relation we performed additional analysis on the AV congruent condition, as it is the condition where the visual input is most relevant and informative for comprehension."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>6) The analysis is done purely in the spectral domain. It would also be of interest to know, e.g., the latency of the visual tracking effect and whether congruent visual information shortens the latency of auditory entrainment. Such analysis would help to link the current study with the previous literature on AV integration.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "Thank you for raising this important and interesting point and we have looked at the latencies of visual and auditory tracking in each condition and the effect of congruence."
                }, 
                {
                    "type": "paragraph", 
                    "text": "We first checked the lag between lip (visual) and sound (auditory) speech signals. We performed cross-correlation analysis to estimate the delay between two signals. We computed this cross-correlation for both matching and non-matching signals used in the study. As shown in <a href=\"#fig8\">Author response image 4</a>, visual signals lead auditory signals by about 70 ms (here negative latency indicates lip speech (visual) signals lead sound speech (auditory) signals) for matching signals, however as expected there\u2019s no effect for non-matching signals."
                }, 
                {
                    "type": "image", 
                    "doi": "10.7554/eLife.14521.015", 
                    "id": "fig8", 
                    "label": "Author response image 4.", 
                    "title": "Author response image 4.", 
                    "alt": "", 
                    "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-resp-fig4-v2.jpg", 
                    "filename": "elife-14521-resp-fig4-v2.jpg"
                }, 
                {
                    "type": "paragraph", 
                    "text": "Next, we performed the cross-correlation analysis between speech signals and brain responses to measure visual and auditory tracking lag and how they change across conditions. We computed cross-correlation for both lip speech-visual cortex signals (visual tracking) and sound speech-auditory cortex signals (auditory tracking). Here the visual and auditory cortices were selected from the maximum voxels in the <a href=\"#fig2\">Figure 2F</a> (left visual cortex (MNI coordinates = [-28 -88 8]), right STG (MNI coordinates = [44 -32 16]))."
                }, 
                {
                    "type": "paragraph", 
                    "text": "In <a href=\"#fig9\">Author response image 5</a> the latency indicates the lag between signals in visual or auditory cortex and the lip or sound speech signals (brain signals following the speech signals). The mean lag across conditions is around 100 ms for all conditions as can be expected from studies of temporal response functions (e.g. Crosse et al., 2015). Congruent visual speech reduces the mean lag of auditory entrainment as can be seen in the comparison of \u2018All cong A\u2019 and \u2018All incong A\u2019. This is consistent with previous findings (van Wassenhove et al., 2005). But the difference is not significant and performing this analysis on narrow-band (filtered) data is not without problems. We therefore prefer to address this question in a separate study."
                }, 
                {
                    "type": "image", 
                    "doi": "10.7554/eLife.14521.016", 
                    "id": "fig9", 
                    "label": "Author response image 5.", 
                    "title": "Author response image 5.", 
                    "alt": "", 
                    "uri": "https://publishing-cdn.elifesciences.org/14521/elife-14521-resp-fig5-v2.jpg", 
                    "filename": "elife-14521-resp-fig5-v2.jpg"
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>7) Although the relationship shown in Figure 5 between coherence in the left motor cortex and speech comprehension is very interesting, this does not show that the \"motor cortex component contributes to speech comprehension\" (Discussion), which would require demonstrating that disruption of activity in motor cortex affects speech comprehension.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "We agree with the reviewers that this statement was too strong since we did not demonstrate a causal contribution of activity in the left motor cortex to comprehension. We have therefore rephrased this by stating \u201centrainment in motor regions correlates with speech comprehension\u201d (Discussion)."
                }, 
                {
                    "type": "paragraph", 
                    "text": "<i>8) Does the motor cortex activity include the lip regions? Please provide appropriate citations to justify this.</i>"
                }, 
                {
                    "type": "paragraph", 
                    "text": "The area in the motor cortex showing significant lip-brain coherence (now <a href=\"#fig3s1\">Figure 3\u2014figure supplement 1C, D</a>) is consistent with the lip/tongue representation in motor studies (see for example Giraud et al., 2007; Simonyan and Horwitz, 2011). In addition, a second area in left motor cortex emerged from the partial coherence analysis. This area was found in upper strip of primary motor cortex (maximum MNI coordinates in the partial coherence results for congruence effect (AV cong &gt; All incong in <a href=\"#fig3\">Figure 3C</a>): [-52 -8 56]; for attention effect (AV cong &gt; All cong in <a href=\"#fig3\">Figure 3D</a>: [-44 -16 56]; both clusters are extending over 10 mm radius). This is consistent with our previous finding from a different study that the same area exerts top-down control of left auditory cortex at the theta frequency band (Park et al., 2015). Although this second area appears to be located more superior compared to the classical lip/tongue motor representation, it still overlaps with motor areas that have been reported in fMRI studies of lip movements (Morillon et al., 2010; Table 1 in Fukunaga et al., 2009; lip panel in <a href=\"#fig2\">Figure 2C</a> in Lotze et al., 2000) and articulatory movements of the lips (<a href=\"#fig3\">Figure 3</a> in Pulvermuller et al., 2006; this region was found to have causal influence on comprehending single spoken words in the TMS study (Schomers et al., 2015)). Further, this area is also consistent with areas observed in the study demonstrating essential role of motor cortex by disrupting it with TMS which leads to impaired speech perception (phonetic discrimination task) (Table 1 in Meister et al., 2007). Thus, we think that the motor cortex found by partial coherence analysis plays an important role in speech perception as we suggested in the Discussion (subsection \u201cMotor areas are entrained by speaking lips\u201d)."
                }
            ]
        }
    }
}