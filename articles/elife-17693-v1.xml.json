{
    "journal": {
        "id": "eLife", 
        "title": "eLife", 
        "issn": "2050-084X"
    }, 
    "snippet": {
        "-meta": {
            "location": "https://raw.githubusercontent.com/elifesciences/elife-article-xml/5aac79759a440b88c82b263dbd866793307e5300/articles/elife-17693-v1.xml"
        }, 
        "-history": {
            "received": "2016-06-06T00:00:00Z", 
            "accepted": "2016-06-06T00:00:00Z"
        }, 
        "status": "vor", 
        "id": "17693", 
        "version": 1, 
        "type": "insight", 
        "doi": "10.7554/eLife.17693", 
        "authorLine": "Gregory B Cogan", 
        "title": "I see what you are saying", 
        "titlePrefix": "Language", 
        "published": "2016-06-09T00:00:00Z", 
        "versionDate": "2016-06-09T00:00:00Z", 
        "volume": 5, 
        "elocationId": "e17693", 
        "pdf": "https://publishing-cdn.elifesciences.org/17693/elife-17693-v1.pdf", 
        "subjects": [
            {
                "id": "neuroscience", 
                "name": "Neuroscience"
            }
        ], 
        "researchOrganisms": [
            "Human"
        ], 
        "abstract": {
            "content": [
                {
                    "type": "paragraph", 
                    "text": "The motor cortex in the brain tracks lip movements to help with speech perception."
                }
            ]
        }, 
        "copyright": {
            "license": "CC-BY-4.0", 
            "holder": "Cogan", 
            "statement": "This article is distributed under the terms of the <a href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License</a>, which permits unrestricted use and redistribution provided that the original author and source are credited."
        }, 
        "authors": [
            {
                "type": "person", 
                "name": {
                    "preferred": "Gregory B Cogan", 
                    "index": "Cogan, Gregory B"
                }, 
                "orcid": "0000-0003-1557-6507", 
                "affiliations": [
                    {
                        "name": [
                            "Department of Biomedical Engineering", 
                            "Duke University"
                        ], 
                        "address": {
                            "formatted": [
                                "Durham", 
                                "United States"
                            ], 
                            "components": {
                                "locality": [
                                    "Durham"
                                ], 
                                "country": "United States"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "gregory.cogan@duke.edu"
                ], 
                "competingInterests": "The author declares that no competing interests exist."
            }
        ]
    }, 
    "article": {
        "-meta": {
            "location": "https://raw.githubusercontent.com/elifesciences/elife-article-xml/5aac79759a440b88c82b263dbd866793307e5300/articles/elife-17693-v1.xml", 
            "patched": true
        }, 
        "-history": {
            "received": "2016-06-06T00:00:00Z", 
            "accepted": "2016-06-06T00:00:00Z"
        }, 
        "status": "vor", 
        "id": "17693", 
        "version": 1, 
        "type": "insight", 
        "doi": "10.7554/eLife.17693", 
        "authorLine": "Gregory B Cogan", 
        "title": "I see what you are saying", 
        "titlePrefix": "Language", 
        "published": "2016-06-09T00:00:00Z", 
        "versionDate": "2016-06-09T00:00:00Z", 
        "volume": 5, 
        "elocationId": "e17693", 
        "pdf": "https://publishing-cdn.elifesciences.org/17693/elife-17693-v1.pdf", 
        "subjects": [
            {
                "id": "neuroscience", 
                "name": "Neuroscience"
            }
        ], 
        "researchOrganisms": [
            "Human"
        ], 
        "abstract": {
            "content": [
                {
                    "type": "paragraph", 
                    "text": "The motor cortex in the brain tracks lip movements to help with speech perception."
                }
            ]
        }, 
        "copyright": {
            "license": "CC-BY-4.0", 
            "holder": "Cogan", 
            "statement": "This article is distributed under the terms of the <a href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License</a>, which permits unrestricted use and redistribution provided that the original author and source are credited."
        }, 
        "authors": [
            {
                "type": "person", 
                "name": {
                    "preferred": "Gregory B Cogan", 
                    "index": "Cogan, Gregory B"
                }, 
                "orcid": "0000-0003-1557-6507", 
                "affiliations": [
                    {
                        "name": [
                            "Department of Biomedical Engineering", 
                            "Duke University"
                        ], 
                        "address": {
                            "formatted": [
                                "Durham", 
                                "United States"
                            ], 
                            "components": {
                                "locality": [
                                    "Durham"
                                ], 
                                "country": "United States"
                            }
                        }
                    }
                ], 
                "emailAddresses": [
                    "gregory.cogan@duke.edu"
                ], 
                "competingInterests": "The author declares that no competing interests exist."
            }
        ], 
        "keywords": [
            "lip movements", 
            "speech", 
            "language", 
            "oscillations", 
            "magnetoencephalography", 
            "electroencephalography"
        ], 
        "-related-articles-internal": [
            "14521"
        ], 
        "-related-articles-external": [], 
        "body": [
            {
                "type": "section", 
                "id": "s0", 
                "title": "Main text", 
                "content": [
                    {
                        "type": "paragraph", 
                        "text": "In the mid-1940s, the psychologist Alvin Liberman went to work with Franklin Cooper at the\u00a0Haskins Laboratories in New Haven, Connecticut. He initially set out to create a device to turn printed letters into sounds so that blind people could \u2018hear\u2019 written texts (<a href=\"#bib5\">Liberman, 1996</a>). His first foray involved shining a light through a slit onto the page in order to convert the lines of each letter into light and then into frequencies of sound. Liberman and colleagues reasoned that with enough training, blind users would be able to learn these arbitrary letter-sound pairs and so be able to understand the text."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "The device was a spectacular failure: the users performed slowly and inaccurately. This led Liberman and colleagues to the realization that speech is not an arbitrary sequence of sounds, but a specific human code. They argued that the key to this code was the link between the speech sounds a person hears and the motor actions they make in order to speak. This important work led to decades of further research and helped lay the foundation for the psychological and neuroscientific study of speech."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "When we watch and listen to someone speak, our brain\u00a0combines\u00a0the visual information of the movement of the speaker\u2019s mouth with the speech sounds that are produced by this movement (<a href=\"#bib8\">McGurk and MacDonald, 1976</a>). One of the core problems that researchers in this field are investigating is how these different sets of information are integrated to allow us to understand speech. Now, in eLife, Hyojin Park, Christoph Kayser, Gregor Thut and Joachim Gross of the University of Glasgow report that they have studied this integration by using a technique called magnetoencephalography to record the magnetic fields that are generated by the electrical currents of the brain (<a href=\"#bib9\">Park et al., 2016</a>)."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "Park et al. presented volunteers with\u00a0audio-visual clips of naturalistic speech and then asked them to complete a short questionnaire about the speech they heard and saw. In some cases, these clips were manipulated so that the audio did not match the video. In other cases, Park et al. presented a different speech signal to each ear and asked the volunteers to pay attention to just one signal. By analyzing these combinations, they could separate the brain activity that is associated with watching someone speak from the activity that processes the speech sounds themselves."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "Park et al. found that a part of the continuous speech stream called the envelope, which is the slow rising and falling in the amplitude of the speech, was tracked in auditory areas of the brain (<a href=\"#fig1\">Figure 1</a>). Conversely, the visual cortex tracked mouth movements. These results are a good replication and extension of previous data recorded from both the auditory\u00a0domain (<a href=\"#bib2\">Cogan and Poeppel, 2011</a>; <a href=\"#bib4\">Gross et al., 2013</a>; <a href=\"#bib6\">Luo and Poeppel, 2007</a>) and the visual domain (<a href=\"#bib7\">Luo et al., 2010</a>; <a href=\"#bib10\">Zion Golumbic et al., 2013</a>). However, Park et al. extended these findings by asking: what role does tracking the lip movements of a speaker play in speech perception?"
                    }, 
                    {
                        "type": "image", 
                        "id": "fig1", 
                        "label": "Figure 1.", 
                        "title": "A proposed model for the role of the motor system in speech perception.", 
                        "caption": [
                            {
                                "type": "paragraph", 
                                "text": "A person produces speech by the coordinated movement of their articulatory system. The listener hears the sound (black line) and sees the mouth of the speaker open and close (represented by blue line). Some of the information in the sound is contained within the speech envelope (green line). The auditory regions of the brain (green circle) track the speech envelope, while the visual system (blue circle) tracks the visual movements of the mouth. The motor system (red circle) then decodes the intended mouth movement and integrates this with the response of the auditory regions to the incoming sounds."
                            }
                        ], 
                        "alt": "", 
                        "uri": "https://publishing-cdn.elifesciences.org/17693/elife-17693-fig1-v1.jpg", 
                        "filename": "elife-17693-fig1-v1.jpg"
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "To learn more about which parts of the brain track the lip movements of the speaker, Park et al. performed a partial regression on the lip movement, envelope and brain activity data to remove the response to sound and focus on just the effect of tracking the lip movements. This revealed two areas of the brain that actively track lip movements during speech. The first area, as found by previous researchers, was the visual cortex. This presumably tracks the lips as a visual signal. The second area was the left motor cortex."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "To further establish the role of the motor cortex during speech perception, Park et al. examined the comprehension scores from the questionnaire. These scores could be predicted from the extent to which neural activity in the motor cortex synchronized with the lip movements observed by the participant: higher scores correlated with a higher degree of synchronization. This suggests that the ability of the motor cortex to track lip movements is important for understanding audiovisual speech, suggesting a new role for the motor system in speech perception. Park et al. interpret this finding to suggest that the motor system helps to predict the upcoming sound signal by simulating the speaker\u2019s intended mouth movement (<a href=\"#bib1\">Arnal and Giraud, 2012</a>; <a href=\"#fig1\">Figure 1</a>)."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "While this is an important first step, it is still not clear how the lip movement tracked by the motor cortex is integrated with the response of auditory regions of the brain to speech sounds. Are mouth movements tracked specifically for ambiguous or difficult stimuli (<a href=\"#bib3\">Du et al., 2014</a>) or is this tracking necessary for perceiving speech generally? Future work will hopefully clarify the specifics of this mechanism."
                    }, 
                    {
                        "type": "paragraph", 
                        "text": "It is interesting and somewhat ironic that the motor cortex tracks the visual signals of mouth movement, given the early (and unsuccessful) efforts of Liberman and colleagues to help the blind \u2018hear\u2019 written\u00a0texts. Indeed, just as these early researchers proposed, it seems that the link between the motor and auditory system is a key to understanding how speech is represented in the brain."
                    }
                ]
            }
        ], 
        "references": [
            {
                "type": "journal", 
                "id": "bib1", 
                "date": "2012", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "LH Arnal", 
                            "index": "Arnal, LH"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AL Giraud", 
                            "index": "Giraud, AL"
                        }
                    }
                ], 
                "articleTitle": "Cortical oscillations and sensory predictions", 
                "journal": "Trends in Cognitive Sciences", 
                "volume": "16", 
                "pages": {
                    "first": "390", 
                    "last": "398", 
                    "range": "390\u2013398"
                }, 
                "doi": "10.1016/j.tics.2012.05.003"
            }, 
            {
                "type": "journal", 
                "id": "bib2", 
                "date": "2011", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "GB Cogan", 
                            "index": "Cogan, GB"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "A mutual information analysis of neural coding of speech by low-frequency MEG phase information", 
                "journal": "Journal of Neurophysiology", 
                "volume": "106", 
                "pages": {
                    "first": "554", 
                    "last": "563", 
                    "range": "554\u2013563"
                }, 
                "doi": "10.1152/jn.00075.2011"
            }, 
            {
                "type": "journal", 
                "id": "bib3", 
                "date": "2014", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Y Du", 
                            "index": "Du, Y"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "BR Buchsbaum", 
                            "index": "Buchsbaum, BR"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CL Grady", 
                            "index": "Grady, CL"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Alain", 
                            "index": "Alain, C"
                        }
                    }
                ], 
                "articleTitle": "Noise differentially impacts phoneme representations in the auditory and speech motor systems", 
                "journal": "Proceedings of the National Academy of Sciences of the United States of America", 
                "volume": "111", 
                "pages": {
                    "first": "7126", 
                    "last": "7131", 
                    "range": "7126\u20137131"
                }, 
                "doi": "10.1073/pnas.1318738111"
            }, 
            {
                "type": "journal", 
                "id": "bib4", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "N Hoogenboom", 
                            "index": "Hoogenboom, N"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Thut", 
                            "index": "Thut, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Schyns", 
                            "index": "Schyns, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Panzeri", 
                            "index": "Panzeri, S"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "P Belin", 
                            "index": "Belin, P"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "S Garrod", 
                            "index": "Garrod, S"
                        }
                    }
                ], 
                "articleTitle": "Speech rhythms and multiplexed oscillatory sensory coding in the human brain", 
                "journal": "PLoS Biology", 
                "volume": "11", 
                "pages": "e1001752", 
                "doi": "10.1371/journal.pbio.1001752"
            }, 
            {
                "type": "book", 
                "id": "bib5", 
                "date": "1996", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "AM Liberman", 
                            "index": "Liberman, AM"
                        }
                    }
                ], 
                "bookTitle": "Speech: A Special Code", 
                "publisher": {
                    "name": [
                        "MIT Press"
                    ], 
                    "address": {
                        "formatted": [
                            "Cambridge, MA"
                        ], 
                        "components": {
                            "locality": [
                                "Cambridge, MA"
                            ]
                        }
                    }
                }, 
                "edition": "1st Ed"
            }, 
            {
                "type": "journal", 
                "id": "bib6", 
                "date": "2007", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Luo", 
                            "index": "Luo, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex", 
                "journal": "Neuron", 
                "volume": "54", 
                "pages": {
                    "first": "1001", 
                    "last": "1010", 
                    "range": "1001\u20131010"
                }, 
                "doi": "10.1016/j.neuron.2007.06.004"
            }, 
            {
                "type": "journal", 
                "id": "bib7", 
                "date": "2010", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Luo", 
                            "index": "Luo, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "Z Liu", 
                            "index": "Liu, Z"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation", 
                "journal": "PLoS Biology", 
                "volume": "8", 
                "pages": "e1000445", 
                "doi": "10.1371/journal.pbio.1000445"
            }, 
            {
                "type": "journal", 
                "id": "bib8", 
                "date": "1976", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H McGurk", 
                            "index": "McGurk, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J MacDonald", 
                            "index": "MacDonald, J"
                        }
                    }
                ], 
                "articleTitle": "Hearing lips and seeing voices", 
                "journal": "Nature", 
                "volume": "264", 
                "pages": {
                    "first": "746", 
                    "last": "748", 
                    "range": "746\u2013748"
                }, 
                "doi": "10.1038/264746a0"
            }, 
            {
                "type": "journal", 
                "id": "bib9", 
                "date": "2016", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "H Park", 
                            "index": "Park, H"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "C Kayser", 
                            "index": "Kayser, C"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "G Thut", 
                            "index": "Thut, G"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "J Gross", 
                            "index": "Gross, J"
                        }
                    }
                ], 
                "articleTitle": "Lip movements entrain the observers\u2019 low-frequency brain oscillations to facilitate speech intelligibility", 
                "journal": "eLife", 
                "volume": "5", 
                "pages": "e14521", 
                "doi": "10.7554/eLife.14521"
            }, 
            {
                "type": "journal", 
                "id": "bib10", 
                "date": "2013", 
                "authors": [
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "E Zion Golumbic", 
                            "index": "Zion Golumbic, E"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "GB Cogan", 
                            "index": "Cogan, GB"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "CE Schroeder", 
                            "index": "Schroeder, CE"
                        }
                    }, 
                    {
                        "type": "person", 
                        "name": {
                            "preferred": "D Poeppel", 
                            "index": "Poeppel, D"
                        }
                    }
                ], 
                "articleTitle": "Visual input enhances selective speech envelope tracking in auditory cortex at a \"cocktail party\"", 
                "journal": "Journal of Neuroscience", 
                "volume": "33", 
                "pages": {
                    "first": "1417", 
                    "last": "1426", 
                    "range": "1417\u20131426"
                }, 
                "doi": "10.1523/JNEUROSCI.3675-12.2013"
            }
        ], 
        "stage": "published", 
        "statusDate": "2099-01-01T00:00:00Z"
    }
}